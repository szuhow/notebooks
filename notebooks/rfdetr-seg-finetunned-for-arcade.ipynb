{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhxdEkemS29m"
   },
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSbiL1I6T8JY"
   },
   "source": [
    "### Install dependencies\n",
    "\n",
    "Installs RF-DETR version 1.4.0 or higher, along with Supervision for benchmarking and Roboflow for pulling datasets and uploading models to the Roboflow platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3CbzMY6wITlr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: 1.4.0 not found\n"
     ]
    }
   ],
   "source": [
    "!pip install rfdetr>=1.4.0 supervision roboflow inference tqdm matplotlib opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==2.8.0+cu128  --index-url https://download.pytorch.org/whl/cu128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/roboflow/rf-detr.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install git+https://github.com/laudominik/torch-arcade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Arcade dataset to coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "from typing import Callable, Optional, Union, Dict, List, Any, Tuple\n",
    "from pathlib import Path\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from torchvision.datasets.utils import download_and_extract_archive\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools import mask as mask_utils\n",
    "\n",
    "try:\n",
    "    import albumentations as A\n",
    "except ImportError:\n",
    "    A = None\n",
    "\n",
    "try:\n",
    "    import cv2\n",
    "except ImportError:\n",
    "    cv2 = None\n",
    "\n",
    "    \n",
    "def distinguish_side(segments):\n",
    "    \"\"\"Determine side based on vessel segments.\"\"\"\n",
    "    right_segments = {\"1\", \"2\", \"3\", \"4\", \"16a\", \"16b\", \"16c\"}\n",
    "    return \"right\" if any(seg in segments for seg in right_segments) else \"left\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AugmentationParams:\n",
    "    \"\"\"\n",
    "    Parameters for data augmentation during COCO dataset conversion.\n",
    "    All probabilities default to 0 (disabled). Set > 0 to enable.\n",
    "    \"\"\"\n",
    "    # Basic augmentations\n",
    "    apply_augmentation: bool = False\n",
    "    augmentation_factor: int = 1  # Number of augmented copies per original image\n",
    "    \n",
    "    # Flip augmentations\n",
    "    horizontal_flip_p: float = 0.0\n",
    "    vertical_flip_p: float = 0.0\n",
    "    random_rotate_90_p: float = 0.0\n",
    "    \n",
    "    # Rotation\n",
    "    rotate_p: float = 0.0\n",
    "    rotate_limit: int = 15\n",
    "    \n",
    "    # Elastic/distortion augmentations\n",
    "    elastic_transform_p: float = 0.0\n",
    "    elastic_transform_alpha: float = 120.0\n",
    "    elastic_transform_sigma: float = 6.0\n",
    "    \n",
    "    grid_distortion_p: float = 0.0\n",
    "    optical_distortion_p: float = 0.0\n",
    "    optical_distortion_limit: float = 0.05\n",
    "    \n",
    "    # Color/brightness augmentations\n",
    "    random_brc_p: float = 0.0\n",
    "    random_brc_b_limit: float = 0.2\n",
    "    random_brc_c_limit: float = 0.2\n",
    "    \n",
    "    # Noise/blur\n",
    "    gauss_noise_p: float = 0.0\n",
    "    blur_p: float = 0.0\n",
    "    blur_limit: int = 3\n",
    "    \n",
    "    # Spatial augmentations\n",
    "    random_resized_crop_p: float = 0.0\n",
    "    random_resized_crop_scale: Tuple[float, float] = (0.8, 1.0)\n",
    "    random_resized_crop_ratio: Tuple[float, float] = (0.9, 1.1)\n",
    "    \n",
    "    shift_scale_rotate_p: float = 0.0\n",
    "    shift_scale_rotate_shift_limit: float = 0.1\n",
    "    shift_scale_rotate_scale_limit: float = 0.1\n",
    "\n",
    "\n",
    "class ARCADEtoCocoConverter:\n",
    "    \"\"\"\n",
    "    Converts ARCADE segmentation dataset to COCO detection format for RF-DETR.\n",
    "    \n",
    "    This converter reads the ARCADE dataset with segmentation masks and exports\n",
    "    images and annotations in COCO format suitable for object detection training.\n",
    "    Supports optional augmentations during conversion.\n",
    "    \n",
    "    Output structure:\n",
    "        dataset/\n",
    "        ‚îú‚îÄ‚îÄ train/\n",
    "        ‚îÇ   ‚îú‚îÄ‚îÄ _annotations.coco.json\n",
    "        ‚îÇ   ‚îî‚îÄ‚îÄ *.png\n",
    "        ‚îú‚îÄ‚îÄ valid/\n",
    "        ‚îÇ   ‚îú‚îÄ‚îÄ _annotations.coco.json\n",
    "        ‚îÇ   ‚îî‚îÄ‚îÄ *.png\n",
    "        ‚îî‚îÄ‚îÄ test/\n",
    "            ‚îú‚îÄ‚îÄ _annotations.coco.json\n",
    "            ‚îî‚îÄ‚îÄ *.png\n",
    "    \"\"\"\n",
    "    \n",
    "    URL: str = \"https://zenodo.org/records/8386059/files/arcade_challenge_datasets.zip\"\n",
    "    ZIPNAME: str = \"arcade_challenge_datasets.zip\"\n",
    "    FILENAME: str = \"arcade_challenge_datasets\"\n",
    "    \n",
    "    # Mapping from internal split names to output folder names\n",
    "    OUTPUT_SPLIT_NAMES = {\n",
    "        \"train\": \"train\",\n",
    "        \"val\": \"valid\",\n",
    "        \"test\": \"test\",\n",
    "    }\n",
    "    \n",
    "    DATASET_DICT = {\n",
    "        \"segmentation\": {\n",
    "            \"train\": {\n",
    "                \"path\": os.path.join(\"dataset_phase_1\", \"segmentation_dataset\", \"seg_train\"),\n",
    "                \"coco\": \"seg_train.json\",\n",
    "            },\n",
    "            \"val\": {\n",
    "                \"path\": os.path.join(\"dataset_phase_1\", \"segmentation_dataset\", \"seg_val\"),\n",
    "                \"coco\": \"seg_val.json\"\n",
    "            },\n",
    "            \"test\": {\n",
    "                \"path\": os.path.join(\"dataset_final_phase\", \"test_case_segmentation\"),\n",
    "                \"coco\": \"instances_default.json\"\n",
    "            }\n",
    "        },\n",
    "        \"stenosis\": {\n",
    "            \"train\": {\n",
    "                \"path\": os.path.join(\"dataset_phase_1\", \"stenosis_dataset\", \"sten_train\"),\n",
    "                \"coco\": \"sten_train.json\"\n",
    "            },\n",
    "            \"val\": {\n",
    "                \"path\": os.path.join(\"dataset_phase_1\", \"stenosis_dataset\", \"sten_val\"),\n",
    "                \"coco\": \"sten_val.json\"\n",
    "            },\n",
    "            \"test\": {\n",
    "                \"path\": os.path.join(\"dataset_final_phase\", \"test_cases_stenosis\"),\n",
    "                \"coco\": \"instances_default.json\"\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: Union[str, Path],\n",
    "        output_dir: Union[str, Path],\n",
    "        download: bool = False,\n",
    "        task_type: str = \"multiclass\",\n",
    "        min_area: float = 10.0,\n",
    "        copy_images: bool = True,\n",
    "        augmentation_params: Optional[AugmentationParams] = None,\n",
    "        image_size: int = 512,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the ARCADE to COCO converter.\n",
    "        \n",
    "        Args:\n",
    "            root: Root directory containing ARCADE dataset\n",
    "            output_dir: Output directory for COCO format dataset\n",
    "            download: Whether to download ARCADE dataset if not present\n",
    "            task_type: \"binary\", \"multiclass\", or \"stenosis\"\n",
    "            min_area: Minimum bounding box area to include annotation\n",
    "            copy_images: If True, copies images; if False, creates symlinks\n",
    "            augmentation_params: Optional augmentation parameters (only for train)\n",
    "            image_size: Target image size for augmentations\n",
    "        \"\"\"\n",
    "        self.root = Path(root)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.task_type = task_type\n",
    "        self.min_area = min_area\n",
    "        self.copy_images = copy_images\n",
    "        self.augmentation_params = augmentation_params\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # Determine which ARCADE dataset folder to use\n",
    "        if self.task_type == \"stenosis\":\n",
    "            self._dataset_task = \"stenosis\"\n",
    "        else:\n",
    "            self._dataset_task = \"segmentation\"\n",
    "        \n",
    "        # Download if needed\n",
    "        if download:\n",
    "            download_and_extract_archive(self.URL, str(self.root), filename=self.ZIPNAME)\n",
    "        \n",
    "        # Category mapping will be built during conversion\n",
    "        self.category_id_to_class_idx: Dict[int, int] = {}\n",
    "        self.coco_categories: List[Dict[str, Any]] = []\n",
    "        \n",
    "        # Build augmentation pipeline if needed\n",
    "        self.augs = None\n",
    "        if augmentation_params and augmentation_params.apply_augmentation:\n",
    "            if A is None:\n",
    "                raise ImportError(\"Albumentations required for augmentations (pip install albumentations)\")\n",
    "            self.augs = self._build_augmentation_pipeline(augmentation_params)\n",
    "    \n",
    "    def _build_augmentation_pipeline(self, p: AugmentationParams) -> Any:\n",
    "        \"\"\"Build albumentations pipeline for bounding box augmentation.\"\"\"\n",
    "        aug_list: List[Any] = []\n",
    "        \n",
    "        if p.horizontal_flip_p > 0:\n",
    "            aug_list.append(A.HorizontalFlip(p=p.horizontal_flip_p))\n",
    "        if p.vertical_flip_p > 0:\n",
    "            aug_list.append(A.VerticalFlip(p=p.vertical_flip_p))\n",
    "        if p.random_rotate_90_p > 0:\n",
    "            aug_list.append(A.RandomRotate90(p=p.random_rotate_90_p))\n",
    "        \n",
    "        if p.rotate_p > 0 and p.rotate_limit != 0:\n",
    "            aug_list.append(A.Rotate(\n",
    "                limit=p.rotate_limit, \n",
    "                p=p.rotate_p, \n",
    "                border_mode=0,\n",
    "                interpolation=cv2.INTER_LINEAR if cv2 else 1,\n",
    "            ))\n",
    "\n",
    "        if p.elastic_transform_p > 0:\n",
    "            aug_list.append(\n",
    "                A.ElasticTransform(\n",
    "                    alpha=p.elastic_transform_alpha,\n",
    "                    sigma=p.elastic_transform_sigma,\n",
    "                    p=p.elastic_transform_p,\n",
    "                    border_mode=0,\n",
    "                )\n",
    "            )\n",
    "        if p.grid_distortion_p > 0:\n",
    "            aug_list.append(A.GridDistortion(\n",
    "                p=p.grid_distortion_p, \n",
    "                border_mode=0,\n",
    "            ))\n",
    "        if p.optical_distortion_p > 0:\n",
    "            aug_list.append(\n",
    "                A.OpticalDistortion(\n",
    "                    distort_limit=p.optical_distortion_limit,\n",
    "                    shift_limit=0.05,\n",
    "                    p=p.optical_distortion_p,\n",
    "                )\n",
    "            )\n",
    "        if p.random_brc_p > 0:\n",
    "            aug_list.append(\n",
    "                A.RandomBrightnessContrast(\n",
    "                    brightness_limit=p.random_brc_b_limit,\n",
    "                    contrast_limit=p.random_brc_c_limit,\n",
    "                    p=p.random_brc_p,\n",
    "                )\n",
    "            )\n",
    "        if p.gauss_noise_p > 0:\n",
    "            aug_list.append(A.GaussNoise(p=p.gauss_noise_p))\n",
    "        if p.blur_p > 0:\n",
    "            aug_list.append(A.Blur(blur_limit=int(p.blur_limit), p=p.blur_p))\n",
    "        \n",
    "        if p.random_resized_crop_p > 0:\n",
    "            aug_list.append(\n",
    "                A.RandomResizedCrop(\n",
    "                    size=(self.image_size, self.image_size),\n",
    "                    scale=p.random_resized_crop_scale,\n",
    "                    ratio=p.random_resized_crop_ratio,\n",
    "                    p=p.random_resized_crop_p\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        if p.shift_scale_rotate_p > 0:\n",
    "            aug_list.append(\n",
    "                A.Affine(\n",
    "                    translate_percent={\n",
    "                        \"x\": (-p.shift_scale_rotate_shift_limit, p.shift_scale_rotate_shift_limit),\n",
    "                        \"y\": (-p.shift_scale_rotate_shift_limit, p.shift_scale_rotate_shift_limit)\n",
    "                    },\n",
    "                    scale=(1 - p.shift_scale_rotate_scale_limit, 1 + p.shift_scale_rotate_scale_limit),\n",
    "                    rotate=0,\n",
    "                    p=p.shift_scale_rotate_p\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        if not aug_list:\n",
    "            return None\n",
    "            \n",
    "        return A.Compose(\n",
    "            aug_list,\n",
    "            bbox_params=A.BboxParams(\n",
    "                format='coco',\n",
    "                label_fields=['category_ids'],\n",
    "                min_area=self.min_area,\n",
    "                min_visibility=0.3,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    def _build_category_mapping(self, coco: COCO) -> None:\n",
    "        \"\"\"Build category mapping based on task type.\"\"\"\n",
    "        categories = coco.loadCats(coco.getCatIds())\n",
    "        categories = sorted(categories, key=lambda cat: cat[\"id\"])\n",
    "        \n",
    "        if self.task_type == \"stenosis\":\n",
    "            stenosis_cat = next((cat for cat in categories if cat[\"name\"] == \"stenosis\"), None)\n",
    "            if stenosis_cat is None:\n",
    "                raise ValueError(\"Stenosis category not found in dataset\")\n",
    "            \n",
    "            self.category_id_to_class_idx = {stenosis_cat[\"id\"]: 1}\n",
    "            self.coco_categories = [{\n",
    "                \"id\": 1,\n",
    "                \"name\": \"stenosis\",\n",
    "                \"supercategory\": \"vessel\"\n",
    "            }]\n",
    "            print(f\"Stenosis mode: 1 category (stenosis)\")\n",
    "            \n",
    "        elif self.task_type == \"multiclass\":\n",
    "            EXCLUDED_CATEGORIES = {26}  # stenosis not used in vessel segmentation\n",
    "            \n",
    "            self.category_id_to_class_idx = {}\n",
    "            self.coco_categories = []\n",
    "            contiguous_idx = 1\n",
    "            \n",
    "            for category in categories:\n",
    "                if category[\"id\"] not in EXCLUDED_CATEGORIES:\n",
    "                    self.category_id_to_class_idx[category[\"id\"]] = contiguous_idx\n",
    "                    self.coco_categories.append({\n",
    "                        \"id\": contiguous_idx,\n",
    "                        \"name\": category[\"name\"],\n",
    "                        \"supercategory\": category.get(\"supercategory\", \"vessel\")\n",
    "                    })\n",
    "                    contiguous_idx += 1\n",
    "            \n",
    "            print(f\"Multiclass mode: {len(self.coco_categories)} categories\")\n",
    "            \n",
    "        else:  # binary\n",
    "            self.category_id_to_class_idx = {}\n",
    "            self.coco_categories = [{\n",
    "                \"id\": 1,\n",
    "                \"name\": \"vessel\",\n",
    "                \"supercategory\": \"vessel\"\n",
    "            }]\n",
    "            for category in categories:\n",
    "                if category[\"name\"] != \"stenosis\":\n",
    "                    self.category_id_to_class_idx[category[\"id\"]] = 1\n",
    "            print(f\"Binary mode: 1 category (vessel)\")\n",
    "    \n",
    "    def _mask_to_bbox(self, mask: np.ndarray) -> Optional[List[float]]:\n",
    "        \"\"\"Convert binary mask to bounding box [x, y, width, height].\"\"\"\n",
    "        rows = np.any(mask, axis=1)\n",
    "        cols = np.any(mask, axis=0)\n",
    "        \n",
    "        if not rows.any() or not cols.any():\n",
    "            return None\n",
    "            \n",
    "        rmin, rmax = np.where(rows)[0][[0, -1]]\n",
    "        cmin, cmax = np.where(cols)[0][[0, -1]]\n",
    "        \n",
    "        x, y = float(cmin), float(rmin)\n",
    "        width = float(cmax - cmin + 1)\n",
    "        height = float(rmax - rmin + 1)\n",
    "        \n",
    "        return [x, y, width, height]\n",
    "    \n",
    "    def _mask_to_polygon(self, mask: np.ndarray) -> List[List[float]]:\n",
    "        \"\"\"Convert binary mask to polygon segmentation format.\"\"\"\n",
    "        if cv2 is None:\n",
    "            bbox = self._mask_to_bbox(mask)\n",
    "            if bbox is None:\n",
    "                return []\n",
    "            x, y, w, h = bbox\n",
    "            return [[x, y, x+w, y, x+w, y+h, x, y+h]]\n",
    "        \n",
    "        contours, _ = cv2.findContours(\n",
    "            mask.astype(np.uint8), \n",
    "            cv2.RETR_EXTERNAL, \n",
    "            cv2.CHAIN_APPROX_SIMPLE\n",
    "        )\n",
    "        \n",
    "        polygons = []\n",
    "        for contour in contours:\n",
    "            if contour.shape[0] >= 3:\n",
    "                polygon = contour.flatten().tolist()\n",
    "                if len(polygon) >= 6:\n",
    "                    polygons.append(polygon)\n",
    "        \n",
    "        return polygons\n",
    "    \n",
    "    def _bbox_to_polygon(self, bbox: List[float]) -> List[List[float]]:\n",
    "        \"\"\"Convert bounding box [x, y, width, height] to polygon segmentation format.\n",
    "        \n",
    "        This creates a rectangular polygon from the bbox coordinates.\n",
    "        Used as fallback when real segmentation is not available.\n",
    "        \"\"\"\n",
    "        x, y, w, h = bbox\n",
    "        # Create rectangular polygon: [x1,y1, x2,y1, x2,y2, x1,y2]\n",
    "        polygon = [\n",
    "            float(x), float(y),           # top-left\n",
    "            float(x + w), float(y),       # top-right\n",
    "            float(x + w), float(y + h),   # bottom-right\n",
    "            float(x), float(y + h)        # bottom-left\n",
    "        ]\n",
    "        return [polygon]\n",
    "    \n",
    "    def _compute_area(self, mask: np.ndarray) -> float:\n",
    "        \"\"\"Compute mask area.\"\"\"\n",
    "        return float(np.sum(mask > 0))\n",
    "    \n",
    "    def _apply_augmentation(\n",
    "        self,\n",
    "        image: np.ndarray,\n",
    "        bboxes: List[List[float]],\n",
    "        category_ids: List[int],\n",
    "    ) -> Tuple[np.ndarray, List[List[float]], List[int]]:\n",
    "        \"\"\"Apply augmentation to image and bounding boxes.\"\"\"\n",
    "        if self.augs is None:\n",
    "            return image, bboxes, category_ids\n",
    "        \n",
    "        try:\n",
    "            augmented = self.augs(\n",
    "                image=image,\n",
    "                bboxes=bboxes,\n",
    "                category_ids=category_ids,\n",
    "            )\n",
    "            return augmented['image'], augmented['bboxes'], augmented['category_ids']\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Augmentation failed: {e}\")\n",
    "            return image, bboxes, category_ids\n",
    "    \n",
    "    def convert_split(self, image_set: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Convert a single split (train/val/test) to COCO format.\n",
    "        \n",
    "        Args:\n",
    "            image_set: One of \"train\", \"val\", \"test\"\n",
    "            \n",
    "        Returns:\n",
    "            COCO format dictionary\n",
    "        \"\"\"\n",
    "        task_dict = self.DATASET_DICT[self._dataset_task][image_set]\n",
    "        dataset_dir = self.root / self.FILENAME / task_dict[\"path\"]\n",
    "        coco_file = dataset_dir / \"annotations\" / task_dict[\"coco\"]\n",
    "        image_dir = dataset_dir / \"images\"\n",
    "        \n",
    "        print(f\"\\n[{image_set.upper()}] Loading COCO annotations from {coco_file}\")\n",
    "        coco = COCO(str(coco_file))\n",
    "        \n",
    "        # Build category mapping on first split\n",
    "        if not self.coco_categories:\n",
    "            self._build_category_mapping(coco)\n",
    "        \n",
    "        # Map split name to output folder name (e.g., \"val\" -> \"valid\")\n",
    "        output_split_name = self.OUTPUT_SPLIT_NAMES.get(image_set, image_set)\n",
    "        \n",
    "        # Create output directories\n",
    "        output_image_dir = self.output_dir / output_split_name\n",
    "        output_image_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Initialize COCO output structure\n",
    "        coco_output = {\n",
    "            \"images\": [],\n",
    "            \"annotations\": [],\n",
    "            \"categories\": self.coco_categories\n",
    "        }\n",
    "        \n",
    "        annotation_id = 1\n",
    "        image_id_counter = 1\n",
    "        images_processed = 0\n",
    "        annotations_added = 0\n",
    "        \n",
    "        # Determine augmentation factor\n",
    "        aug_factor = 1\n",
    "        apply_augs = False\n",
    "        if (self.augmentation_params \n",
    "            and self.augmentation_params.apply_augmentation \n",
    "            and self.augs is not None \n",
    "            and image_set == \"train\"):\n",
    "            aug_factor = self.augmentation_params.augmentation_factor\n",
    "            apply_augs = True\n",
    "            print(f\"  Augmentation enabled: {aug_factor}x factor\")\n",
    "        \n",
    "        # Process each image - SORTED by id for consistent ordering\n",
    "        image_list = sorted(coco.dataset['images'], key=lambda x: x['id'])\n",
    "        for img_info in tqdm(image_list, desc=f\"Processing {image_set}\"):\n",
    "            img_filename = img_info['file_name']\n",
    "            img_path = image_dir / img_filename\n",
    "            \n",
    "            if not img_path.exists() or not img_filename.endswith('.png'):\n",
    "                continue\n",
    "            \n",
    "            orig_img_id = img_info['id']\n",
    "            \n",
    "            # Load image\n",
    "            image = np.array(Image.open(img_path).convert('RGB'))\n",
    "            \n",
    "            # Get annotations for this image\n",
    "            ann_ids = coco.getAnnIds(imgIds=orig_img_id)\n",
    "            annotations = coco.loadAnns(ann_ids)\n",
    "            \n",
    "            # Collect bboxes and categories\n",
    "            bboxes = []\n",
    "            category_ids = []\n",
    "            segmentations = []\n",
    "            \n",
    "            for ann in annotations:\n",
    "                category_id = ann[\"category_id\"]\n",
    "                \n",
    "                # Skip if category not in our mapping\n",
    "                if category_id not in self.category_id_to_class_idx:\n",
    "                    continue\n",
    "                \n",
    "                new_category_id = self.category_id_to_class_idx[category_id]\n",
    "                \n",
    "                # Get mask and compute bbox\n",
    "                mask = coco.annToMask(ann)\n",
    "                bbox = self._mask_to_bbox(mask)\n",
    "                \n",
    "                if bbox is None:\n",
    "                    continue\n",
    "                \n",
    "                area = bbox[2] * bbox[3]\n",
    "                if area < self.min_area:\n",
    "                    continue\n",
    "                \n",
    "                bboxes.append(bbox)\n",
    "                category_ids.append(new_category_id)\n",
    "                \n",
    "                # U≈ºyj oryginalnych polygon√≥w je≈õli dostƒôpne, w przeciwnym razie wygeneruj z maski\n",
    "                if \"segmentation\" in ann and isinstance(ann[\"segmentation\"], list) and len(ann[\"segmentation\"]) > 0:\n",
    "                    # Oryginalna segmentacja - ju≈º jest listƒÖ polygon√≥w\n",
    "                    segmentations.append(ann[\"segmentation\"])\n",
    "                else:\n",
    "                    # Wygeneruj z maski\n",
    "                    segmentations.append(self._mask_to_polygon(mask))\n",
    "            \n",
    "            if not bboxes:\n",
    "                continue\n",
    "            \n",
    "            # Process original + augmented versions\n",
    "            for aug_idx in range(aug_factor):\n",
    "                if aug_idx == 0:\n",
    "                    # Original image - use real polygon segmentations\n",
    "                    proc_image = image.copy()\n",
    "                    proc_bboxes = bboxes.copy()\n",
    "                    proc_cats = category_ids.copy()\n",
    "                    proc_segs = segmentations.copy()  # Use real segmentations\n",
    "                    suffix = \"\"\n",
    "                else:\n",
    "                    # Augmented version - generate bbox-derived segmentations\n",
    "                    if not apply_augs:\n",
    "                        continue\n",
    "                    proc_image, proc_bboxes, proc_cats = self._apply_augmentation(\n",
    "                        image.copy(), bboxes.copy(), category_ids.copy()\n",
    "                    )\n",
    "                    # For augmented images, derive segmentation from bbox\n",
    "                    proc_segs = [self._bbox_to_polygon(bbox) for bbox in proc_bboxes]\n",
    "                    suffix = f\"_aug{aug_idx}\"\n",
    "                \n",
    "                if not proc_bboxes:\n",
    "                    continue\n",
    "                \n",
    "                # Save image\n",
    "                base_name = img_filename.replace('.png', '')\n",
    "                new_filename = f\"{base_name}{suffix}.png\"\n",
    "                output_img_path = output_image_dir / new_filename\n",
    "                \n",
    "                Image.fromarray(proc_image).save(output_img_path)\n",
    "                \n",
    "                # Add image info\n",
    "                h, w = proc_image.shape[:2]\n",
    "                coco_output[\"images\"].append({\n",
    "                    \"id\": image_id_counter,\n",
    "                    \"file_name\": new_filename,\n",
    "                    \"width\": w,\n",
    "                    \"height\": h\n",
    "                })\n",
    "                \n",
    "                # Grupuj adnotacje wed≈Çug category_id\n",
    "                # COCO format: jedna adnotacja na (image_id, category_id) z wieloma polygonami\n",
    "                category_to_data = {}\n",
    "                for bbox, cat_id, seg in zip(proc_bboxes, proc_cats, proc_segs):\n",
    "                    valid_seg = seg if seg and len(seg) > 0 else self._bbox_to_polygon(bbox)\n",
    "                    \n",
    "                    if cat_id not in category_to_data:\n",
    "                        category_to_data[cat_id] = {\n",
    "                            \"polygons\": [],\n",
    "                            \"total_area\": 0,\n",
    "                            \"min_x\": float('inf'),\n",
    "                            \"min_y\": float('inf'),\n",
    "                            \"max_x\": 0,\n",
    "                            \"max_y\": 0\n",
    "                        }\n",
    "                    \n",
    "                    # Dodaj wszystkie polygony z tej segmentacji\n",
    "                    category_to_data[cat_id][\"polygons\"].extend(valid_seg)\n",
    "                    category_to_data[cat_id][\"total_area\"] += bbox[2] * bbox[3]\n",
    "                    \n",
    "                    # Aktualizuj combined bbox\n",
    "                    category_to_data[cat_id][\"min_x\"] = min(category_to_data[cat_id][\"min_x\"], bbox[0])\n",
    "                    category_to_data[cat_id][\"min_y\"] = min(category_to_data[cat_id][\"min_y\"], bbox[1])\n",
    "                    category_to_data[cat_id][\"max_x\"] = max(category_to_data[cat_id][\"max_x\"], bbox[0] + bbox[2])\n",
    "                    category_to_data[cat_id][\"max_y\"] = max(category_to_data[cat_id][\"max_y\"], bbox[1] + bbox[3])\n",
    "                \n",
    "                # Utw√≥rz jednƒÖ adnotacjƒô per category z wszystkimi polygonami\n",
    "                for cat_id, data in category_to_data.items():\n",
    "                    if data[\"polygons\"]:\n",
    "                        combined_bbox = [\n",
    "                            data[\"min_x\"],\n",
    "                            data[\"min_y\"],\n",
    "                            data[\"max_x\"] - data[\"min_x\"],\n",
    "                            data[\"max_y\"] - data[\"min_y\"]\n",
    "                        ]\n",
    "                        \n",
    "                        coco_ann = {\n",
    "                            \"id\": annotation_id,\n",
    "                            \"image_id\": image_id_counter,\n",
    "                            \"category_id\": cat_id,\n",
    "                            \"bbox\": combined_bbox,\n",
    "                            \"area\": data[\"total_area\"],\n",
    "                            \"iscrowd\": 0,\n",
    "                            \"segmentation\": data[\"polygons\"],  # Lista wszystkich polygon√≥w tej klasy\n",
    "                        }\n",
    "                        coco_output[\"annotations\"].append(coco_ann)\n",
    "                        annotation_id += 1\n",
    "                        annotations_added += 1\n",
    "                \n",
    "                image_id_counter += 1\n",
    "            \n",
    "            images_processed += 1\n",
    "        \n",
    "        print(f\"[{image_set.upper()}] Processed {images_processed} source images\")\n",
    "        print(f\"[{image_set.upper()}] Output: {len(coco_output['images'])} images, {annotations_added} annotations\")\n",
    "        \n",
    "        return coco_output\n",
    "    \n",
    "    def convert(self, image_sets: List[str] = [\"train\", \"val\"]) -> None:\n",
    "        \"\"\"\n",
    "        Convert ARCADE dataset to COCO format.\n",
    "        \n",
    "        Args:\n",
    "            image_sets: List of splits to convert (e.g., [\"train\", \"val\", \"test\"])\n",
    "        \"\"\"\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"Converting ARCADE to COCO format\")\n",
    "        print(f\"  Task type: {self.task_type}\")\n",
    "        print(f\"  Output dir: {self.output_dir}\")\n",
    "        print(f\"  Splits: {image_sets}\")\n",
    "        \n",
    "        for image_set in image_sets:\n",
    "            coco_data = self.convert_split(image_set)\n",
    "            \n",
    "            # Map split name to output folder name (e.g., \"val\" -> \"valid\")\n",
    "            output_split_name = self.OUTPUT_SPLIT_NAMES.get(image_set, image_set)\n",
    "            \n",
    "            # Save COCO JSON\n",
    "            output_json = self.output_dir / output_split_name / \"_annotations.coco.json\"\n",
    "            with open(output_json, 'w') as f:\n",
    "                json.dump(coco_data, f, indent=2)\n",
    "            \n",
    "            print(f\"  Saved: {output_json}\")\n",
    "        \n",
    "        # Create dataset info file\n",
    "        info = {\n",
    "            \"description\": f\"ARCADE dataset converted to COCO format ({self.task_type} mode)\",\n",
    "            \"task_type\": self.task_type,\n",
    "            \"source\": \"ARCADE Challenge Dataset\",\n",
    "            \"categories\": self.coco_categories,\n",
    "            \"splits\": image_sets,\n",
    "            \"augmentation\": self.augmentation_params.__dict__ if self.augmentation_params else None,\n",
    "        }\n",
    "        info_file = self.output_dir / \"dataset_info.json\"\n",
    "        with open(info_file, 'w') as f:\n",
    "            json.dump(info, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nConversion complete!\")\n",
    "        print(f\"  Dataset info: {info_file}\")\n",
    "        print(f\"  Categories: {len(self.coco_categories)}\")\n",
    "\n",
    "\n",
    "# Convenience function for quick conversion\n",
    "def convert_arcade_to_coco(\n",
    "    arcade_root: str,\n",
    "    output_dir: str,\n",
    "    task_type: str = \"multiclass\",\n",
    "    image_sets: List[str] = [\"train\", \"val\"],\n",
    "    download: bool = False,\n",
    "    min_area: float = 10.0,\n",
    "    augmentation_params: Optional[AugmentationParams] = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Convert ARCADE dataset to COCO detection format.\n",
    "    \n",
    "    Output folders: train/, valid/, test/\n",
    "    \n",
    "    Args:\n",
    "        arcade_root: Path to ARCADE dataset root\n",
    "        output_dir: Output directory for COCO format dataset  \n",
    "        task_type: \"binary\", \"multiclass\", or \"stenosis\"\n",
    "        image_sets: Splits to convert (\"train\", \"val\", \"test\")\n",
    "        download: Download dataset if not present\n",
    "        min_area: Minimum bbox area to include\n",
    "        augmentation_params: Optional augmentation parameters (only for train)\n",
    "    \"\"\"\n",
    "    converter = ARCADEtoCocoConverter(\n",
    "        root=arcade_root,\n",
    "        output_dir=output_dir,\n",
    "        download=download,\n",
    "        task_type=task_type,\n",
    "        min_area=min_area,\n",
    "        augmentation_params=augmentation_params,\n",
    "    )\n",
    "    converter.convert(image_sets=image_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert ARCADE Dataset to COCO Format\n",
    "\n",
    "Use the `ARCADEtoCocoConverter` class to convert ARCADE segmentation masks to COCO detection format suitable for RF-DETR training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Convert ARCADE dataset to COCO format for RF-DETR\n",
    "\n",
    "# Define paths\n",
    "ARCADE_ROOT = \"./dataset/arcade_challenge_datasets\"  # Path to extracted ARCADE dataset\n",
    "OUTPUT_DIR = \"./arcade_coco_detection2\"  # Output directory for COCO format\n",
    "\n",
    "# Option 1: Use convenience function\n",
    "convert_arcade_to_coco(\n",
    "    arcade_root=\"./dataset\",  # Parent folder containing arcade_challenge_datasets\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    task_type=\"multiclass\",  # \"binary\", \"multiclass\", or \"stenosis\"\n",
    "    image_sets=[\"train\", \"val\", \"test\"],  # Include test set\n",
    "    download=False,  # Set True to download ARCADE if not present\n",
    "    min_area=0.0,  # Filter out tiny annotations\n",
    ")\n",
    "\n",
    "# Option 2: Use class directly for more control\n",
    "# converter = ARCADEtoCocoConverter(\n",
    "#     root=\"./dataset\",\n",
    "#     output_dir=OUTPUT_DIR,\n",
    "#     task_type=\"multiclass\",\n",
    "#     min_area=10.0,\n",
    "#     copy_images=True,  # False to use symlinks instead\n",
    "# )\n",
    "# converter.convert(image_sets=[\"train\", \"val\", \"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Statistics\n",
    "\n",
    "Analyze category distribution and annotation statistics in the converted COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def analyze_coco_dataset(dataset_dir: str, splits: list = [\"train\", \"val\"]):\n",
    "    \"\"\"\n",
    "    Analyze COCO dataset and display category statistics.\n",
    "    \n",
    "    Args:\n",
    "        dataset_dir: Path to COCO format dataset\n",
    "        splits: List of splits to analyze\n",
    "    \"\"\"\n",
    "    all_stats = {}\n",
    "    \n",
    "    for split in splits:\n",
    "        ann_file = os.path.join(dataset_dir, split, \"_annotations.coco.json\")\n",
    "        if not os.path.exists(ann_file):\n",
    "            print(f\"‚ö†Ô∏è {split}: annotation file not found at {ann_file}\")\n",
    "            continue\n",
    "            \n",
    "        with open(ann_file, 'r') as f:\n",
    "            coco_data = json.load(f)\n",
    "        \n",
    "        # Build category id to name mapping\n",
    "        cat_id_to_name = {cat['id']: cat['name'] for cat in coco_data['categories']}\n",
    "        \n",
    "        # Count annotations per category\n",
    "        category_counts = defaultdict(int)\n",
    "        category_areas = defaultdict(list)\n",
    "        category_bbox_sizes = defaultdict(list)\n",
    "        \n",
    "        for ann in coco_data['annotations']:\n",
    "            cat_id = ann['category_id']\n",
    "            cat_name = cat_id_to_name.get(cat_id, f\"unknown_{cat_id}\")\n",
    "            category_counts[cat_name] += 1\n",
    "            \n",
    "            # Track areas\n",
    "            if 'area' in ann:\n",
    "                category_areas[cat_name].append(ann['area'])\n",
    "            \n",
    "            # Track bbox sizes\n",
    "            if 'bbox' in ann:\n",
    "                w, h = ann['bbox'][2], ann['bbox'][3]\n",
    "                category_bbox_sizes[cat_name].append((w, h))\n",
    "        \n",
    "        # Calculate statistics\n",
    "        stats = {\n",
    "            'num_images': len(coco_data['images']),\n",
    "            'num_annotations': len(coco_data['annotations']),\n",
    "            'num_categories': len(coco_data['categories']),\n",
    "            'categories': coco_data['categories'],\n",
    "            'category_counts': dict(category_counts),\n",
    "            'category_areas': {k: np.array(v) for k, v in category_areas.items()},\n",
    "            'category_bbox_sizes': {k: np.array(v) for k, v in category_bbox_sizes.items()},\n",
    "        }\n",
    "        all_stats[split] = stats\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìä {split.upper()} SPLIT STATISTICS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"  üìÅ Images: {stats['num_images']}\")\n",
    "        print(f\"  üè∑Ô∏è  Annotations: {stats['num_annotations']}\")\n",
    "        print(f\"  üìÇ Categories: {stats['num_categories']}\")\n",
    "        print(f\"  üìà Avg annotations/image: {stats['num_annotations']/max(stats['num_images'],1):.2f}\")\n",
    "        \n",
    "        print(f\"\\n  Category Distribution:\")\n",
    "        print(f\"  {'-'*50}\")\n",
    "        \n",
    "        # Sort by count descending\n",
    "        sorted_cats = sorted(category_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for cat_name, count in sorted_cats:\n",
    "            pct = 100 * count / stats['num_annotations'] if stats['num_annotations'] > 0 else 0\n",
    "            areas = category_areas[cat_name]\n",
    "            bbox_sizes = category_bbox_sizes[cat_name]\n",
    "            \n",
    "            # Area stats\n",
    "            area_min = np.min(areas) if len(areas) > 0 else 0\n",
    "            area_max = np.max(areas) if len(areas) > 0 else 0\n",
    "            area_mean = np.mean(areas) if len(areas) > 0 else 0\n",
    "            \n",
    "            # Bbox size stats\n",
    "            if len(bbox_sizes) > 0:\n",
    "                w_mean = np.mean([s[0] for s in bbox_sizes])\n",
    "                h_mean = np.mean([s[1] for s in bbox_sizes])\n",
    "                bbox_str = f\"bbox: {w_mean:.0f}x{h_mean:.0f}\"\n",
    "            else:\n",
    "                bbox_str = \"\"\n",
    "            \n",
    "            print(f\"  {cat_name:20s} | {count:5d} ({pct:5.1f}%) | area: {area_min:.0f}-{area_max:.0f} (Œº={area_mean:.0f}) | {bbox_str}\")\n",
    "    \n",
    "    return all_stats\n",
    "\n",
    "\n",
    "def plot_category_distribution(stats: dict, figsize=(14, 6)):\n",
    "    \"\"\"Plot category distribution across splits.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(stats), figsize=figsize)\n",
    "    if len(stats) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, (split, split_stats) in zip(axes, stats.items()):\n",
    "        counts = split_stats['category_counts']\n",
    "        sorted_cats = sorted(counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Take top 25 categories for readability\n",
    "        if len(sorted_cats) > 25:\n",
    "            sorted_cats = sorted_cats[:25]\n",
    "        \n",
    "        names = [c[0] for c in sorted_cats]\n",
    "        values = [c[1] for c in sorted_cats]\n",
    "        \n",
    "        colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(names)))\n",
    "        \n",
    "        bars = ax.barh(names, values, color=colors)\n",
    "        ax.set_xlabel('Number of Annotations')\n",
    "        ax.set_title(f'{split.upper()} - Category Distribution\\n({split_stats[\"num_images\"]} images, {split_stats[\"num_annotations\"]} annotations)')\n",
    "        ax.invert_yaxis()\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, val in zip(bars, values):\n",
    "            ax.text(val + max(values)*0.01, bar.get_y() + bar.get_height()/2, \n",
    "                   f'{val}', va='center', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_bbox_size_distribution(stats: dict, split: str = \"train\", figsize=(12, 5)):\n",
    "    \"\"\"Plot bounding box size distribution.\"\"\"\n",
    "    \n",
    "    if split not in stats:\n",
    "        print(f\"Split '{split}' not found in stats\")\n",
    "        return\n",
    "    \n",
    "    split_stats = stats[split]\n",
    "    bbox_sizes = split_stats['category_bbox_sizes']\n",
    "    \n",
    "    # Aggregate all bbox sizes\n",
    "    all_widths = []\n",
    "    all_heights = []\n",
    "    for sizes in bbox_sizes.values():\n",
    "        if len(sizes) > 0:\n",
    "            all_widths.extend(sizes[:, 0])\n",
    "            all_heights.extend(sizes[:, 1])\n",
    "    \n",
    "    if not all_widths:\n",
    "        print(\"No bbox data available\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "    \n",
    "    # Width distribution\n",
    "    axes[0].hist(all_widths, bins=50, color='steelblue', edgecolor='white', alpha=0.7)\n",
    "    axes[0].axvline(np.mean(all_widths), color='red', linestyle='--', label=f'Mean: {np.mean(all_widths):.1f}')\n",
    "    axes[0].set_xlabel('Bbox Width (px)')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_title(f'{split.upper()} - Bbox Width Distribution')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Height distribution\n",
    "    axes[1].hist(all_heights, bins=50, color='forestgreen', edgecolor='white', alpha=0.7)\n",
    "    axes[1].axvline(np.mean(all_heights), color='red', linestyle='--', label=f'Mean: {np.mean(all_heights):.1f}')\n",
    "    axes[1].set_xlabel('Bbox Height (px)')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].set_title(f'{split.upper()} - Bbox Height Distribution')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # Width vs Height scatter\n",
    "    sample_size = min(5000, len(all_widths))\n",
    "    indices = np.random.choice(len(all_widths), sample_size, replace=False)\n",
    "    axes[2].scatter(\n",
    "        [all_widths[i] for i in indices], \n",
    "        [all_heights[i] for i in indices], \n",
    "        alpha=0.3, s=5, c='purple'\n",
    "    )\n",
    "    axes[2].plot([0, max(all_widths)], [0, max(all_widths)], 'r--', alpha=0.5, label='1:1 ratio')\n",
    "    axes[2].set_xlabel('Width (px)')\n",
    "    axes[2].set_ylabel('Height (px)')\n",
    "    axes[2].set_title(f'{split.upper()} - Bbox Aspect Ratio')\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary stats\n",
    "    print(f\"\\nüìê Bounding Box Statistics ({split.upper()}):\")\n",
    "    print(f\"  Width:  min={np.min(all_widths):.0f}, max={np.max(all_widths):.0f}, mean={np.mean(all_widths):.1f}, std={np.std(all_widths):.1f}\")\n",
    "    print(f\"  Height: min={np.min(all_heights):.0f}, max={np.max(all_heights):.0f}, mean={np.mean(all_heights):.1f}, std={np.std(all_heights):.1f}\")\n",
    "\n",
    "\n",
    "# Analyze the converted dataset\n",
    "# Note: Output folders are train/, valid/, test/\n",
    "stats = analyze_coco_dataset(OUTPUT_DIR, splits=[\"train\", \"valid\", \"test\"])\n",
    "\n",
    "# Plot distributions\n",
    "if stats:\n",
    "    plot_category_distribution(stats)\n",
    "    plot_bbox_size_distribution(stats, split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_9c113E39QP"
   },
   "source": [
    "## Clean GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRE_csKqxC9U"
   },
   "source": [
    "Before benchmarking the model, we need to load the best saved checkpoint. To ensure it fits on the GPU, we first need to free up GPU memory. This involves deleting any remaining references to previously used objects, triggering Python‚Äôs garbage collector, and clearing the CUDA memory cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0TBa42oujIfx"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import weakref\n",
    "\n",
    "def cleanup_gpu_memory(obj=None, verbose: bool = False):\n",
    "\n",
    "    if not torch.cuda.is_available():\n",
    "        if verbose:\n",
    "            print(\"[INFO] CUDA is not available. No GPU cleanup needed.\")\n",
    "        return\n",
    "\n",
    "    def get_memory_stats():\n",
    "        allocated = torch.cuda.memory_allocated()\n",
    "        reserved = torch.cuda.memory_reserved()\n",
    "        return allocated, reserved\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    if verbose:\n",
    "        alloc, reserv = get_memory_stats()\n",
    "        print(f\"[Before] Allocated: {alloc / 1024**2:.2f} MB | Reserved: {reserv / 1024**2:.2f} MB\")\n",
    "\n",
    "    # Ensure we drop all strong references\n",
    "    if obj is not None:\n",
    "        ref = weakref.ref(obj)\n",
    "        del obj\n",
    "        if ref() is not None and verbose:\n",
    "            print(\"[WARNING] Object not fully garbage collected yet.\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    if verbose:\n",
    "        alloc, reserv = get_memory_stats()\n",
    "        print(f\"[After]  Allocated: {alloc / 1024**2:.2f} MB | Reserved: {reserv / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_dvSiE8xUqy"
   },
   "source": [
    "We load the best-performing model from the `checkpoint_best_total.pth` file using the `RFDETRMedium` class. This checkpoint contains the trained weights from our most successful training run. After loading, we call `optimize_for_inference()`, which prepares the model for efficient inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xm-lmRWLswO4"
   },
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "\n",
    "ds = sv.DetectionDataset.from_coco(\n",
    "    images_directory_path=f\"./arcade_coco_dataset_seg/test\",\n",
    "    annotations_path=f\"./arcade_coco_dataset_seg/test/_annotations.coco.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß W≈ÅASNY LOADER COCO Z MASKAMI\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import supervision as sv\n",
    "from pycocotools import mask as mask_utils\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "class COCOSegmentationDataset:\n",
    "    \"\"\"Dataset loader kt√≥ry ≈Çaduje maski segmentacji z COCO polygons.\"\"\"\n",
    "    \n",
    "    def __init__(self, images_dir: str, annotations_path: str):\n",
    "        self.images_dir = images_dir\n",
    "        \n",
    "        with open(annotations_path, 'r') as f:\n",
    "            self.coco_data = json.load(f)\n",
    "        \n",
    "        # Mapowanie image_id -> image_info\n",
    "        self.images = {img['id']: img for img in self.coco_data['images']}\n",
    "        \n",
    "        # Grupuj annotations po image_id\n",
    "        self.annotations_by_image = {}\n",
    "        for ann in self.coco_data['annotations']:\n",
    "            img_id = ann['image_id']\n",
    "            if img_id not in self.annotations_by_image:\n",
    "                self.annotations_by_image[img_id] = []\n",
    "            self.annotations_by_image[img_id].append(ann)\n",
    "        \n",
    "        # Lista image_ids\n",
    "        self.image_ids = list(self.images.keys())\n",
    "        \n",
    "        # Kategorie\n",
    "        self.categories = {cat['id']: cat['name'] for cat in self.coco_data['categories']}\n",
    "        self.classes = [cat['name'] for cat in sorted(self.coco_data['categories'], key=lambda x: x['id'])]\n",
    "        \n",
    "        print(f\"‚úì Loaded {len(self.image_ids)} images with segmentation masks\")\n",
    "        print(f\"‚úì Categories: {self.categories}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        img_info = self.images[img_id]\n",
    "        \n",
    "        # ≈öcie≈ºka do obrazu\n",
    "        img_path = os.path.join(self.images_dir, img_info['file_name'])\n",
    "        \n",
    "        # Za≈Çaduj obraz aby uzyskaƒá wymiary\n",
    "        with Image.open(img_path) as img:\n",
    "            width, height = img.size\n",
    "        \n",
    "        # Pobierz annotations dla tego obrazu\n",
    "        anns = self.annotations_by_image.get(img_id, [])\n",
    "        \n",
    "        if not anns:\n",
    "            # Brak annotations\n",
    "            return img_path, None, sv.Detections.empty()\n",
    "        \n",
    "        # Konwertuj annotations na maski i bboxes\n",
    "        masks = []\n",
    "        bboxes = []\n",
    "        class_ids = []\n",
    "        \n",
    "        for ann in anns:\n",
    "            # Konwertuj polygon na maskƒô\n",
    "            segmentation = ann['segmentation']\n",
    "            if isinstance(segmentation, list) and len(segmentation) > 0:\n",
    "                # Polygon format\n",
    "                mask = self._polygon_to_mask(segmentation, height, width)\n",
    "                masks.append(mask)\n",
    "            elif isinstance(segmentation, dict):\n",
    "                # RLE format\n",
    "                mask = mask_utils.decode(segmentation)\n",
    "                masks.append(mask)\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # Bbox w formacie COCO [x, y, w, h] -> xyxy [x1, y1, x2, y2]\n",
    "            x, y, w, h = ann['bbox']\n",
    "            bboxes.append([x, y, x + w, y + h])\n",
    "            \n",
    "            # Class ID - ZACHOWAJ ORYGINALNE category_id z COCO (1-indexed)\n",
    "            # Model RF-DETR te≈º u≈ºywa 1-indexed, wiƒôc NIE odejmuj 1!\n",
    "            class_ids.append(ann['category_id'])\n",
    "        \n",
    "        if not masks:\n",
    "            return img_path, None, sv.Detections.empty()\n",
    "        \n",
    "        # Utw√≥rz obiekt Detections z maskami\n",
    "        detections = sv.Detections(\n",
    "            xyxy=np.array(bboxes, dtype=np.float32),\n",
    "            mask=np.array(masks, dtype=bool),\n",
    "            class_id=np.array(class_ids, dtype=int)\n",
    "        )\n",
    "        \n",
    "        return img_path, None, detections\n",
    "    \n",
    "    def _polygon_to_mask(self, segmentation, height, width):\n",
    "        \"\"\"Konwertuje polygon(y) COCO na binarnƒÖ maskƒô.\"\"\"\n",
    "        mask = np.zeros((height, width), dtype=np.uint8)\n",
    "        \n",
    "        for polygon in segmentation:\n",
    "            if len(polygon) >= 6:  # Minimum 3 punkty (6 wsp√≥≈Çrzƒôdnych)\n",
    "                pts = np.array(polygon, dtype=np.int32).reshape(-1, 2)\n",
    "                cv2.fillPoly(mask, [pts], 1)\n",
    "        \n",
    "        return mask.astype(bool)\n",
    "\n",
    "# üöÄ Za≈Çaduj dataset z maskami\n",
    "ds = COCOSegmentationDataset(\n",
    "    images_dir=\"./arcade_coco_dataset_seg/test\",\n",
    "    annotations_path=\"./arcade_coco_dataset_seg/test/_annotations.coco.json\"\n",
    ")\n",
    "\n",
    "# Test - sprawd≈∫ czy maski sƒÖ za≈Çadowane\n",
    "path, _, annotations = ds[0]\n",
    "print(f\"\\nüîç Test sample 0:\")\n",
    "print(f\"   Path: {path}\")\n",
    "print(f\"   Has mask: {annotations.mask is not None}\")\n",
    "if annotations.mask is not None:\n",
    "    print(f\"   Mask shape: {annotations.mask.shape}\")\n",
    "    print(f\"   Class IDs: {annotations.class_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBotLMbO-HBa"
   },
   "source": [
    "## Training for segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch_arcade import ARCADESemanticSegmentation\n",
    "import os\n",
    "from pycocotools import mask as mask_utils\n",
    "from zipfile import BadZipFile\n",
    "\n",
    "output_root_dir = \"arcade_coco_dataset_multiclass_seg\"\n",
    "os.makedirs(output_root_dir, exist_ok=True)\n",
    "\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    split_path = os.path.join(output_root_dir, split)\n",
    "    os.makedirs(split_path, exist_ok=True)\n",
    "\n",
    "print(f\"Created directory structure under: {output_root_dir}\")\n",
    "\n",
    "# ‚úÖ RZECZYWISTE NAZWY KLAS - Tƒôtnice wie≈Ñcowe\n",
    "CLASS_ID_TO_LABEL = {\n",
    "    1: \"RCA_prox\",\n",
    "    2: \"RCA_mid\",\n",
    "    3: \"RCA_dist\",\n",
    "    4: \"PDA\",\n",
    "    5: \"LM\",\n",
    "    6: \"LAD_prox\",\n",
    "    7: \"LAD_mid\",\n",
    "    8: \"LAD_apical\",\n",
    "    9: \"D1\",\n",
    "    10: \"D1_branch_9a\",\n",
    "    11: \"D2\",\n",
    "    12: \"D2_branch_10a\",\n",
    "    13: \"LCx_prox_11\",\n",
    "    14: \"OM1_12\",\n",
    "    15: \"OM1_branch_12a\",\n",
    "    16: \"LCx_mid_13\",\n",
    "    17: \"OM2_14\",\n",
    "    18: \"OM2_branch_14a\",\n",
    "    19: \"LCx_dist_15\",\n",
    "    20: \"PLB_16\",\n",
    "    21: \"PLB_branch_16a\",\n",
    "    22: \"PLB_branch_16b\",\n",
    "    23: \"PLB_branch_16c\",\n",
    "    24: \"OM1_branch_12b\",\n",
    "    25: \"OM2_branch_14b\",\n",
    "    26: \"stenosis\",\n",
    "}\n",
    "\n",
    "CLASS_ID_TO_FULL_LABEL = {\n",
    "    1: \"1: RCA prox\",\n",
    "    2: \"2: RCA mid\",\n",
    "    3: \"3: RCA dist\",\n",
    "    4: \"4: PDA\",\n",
    "    5: \"5: LM\",\n",
    "    6: \"6: LAD prox\",\n",
    "    7: \"7: LAD mid\",\n",
    "    8: \"8: LAD apical\",\n",
    "    9: \"9: D1\",\n",
    "    10: \"10: D1 branch (9a)\",\n",
    "    11: \"11: D2\",\n",
    "    12: \"12: D2 branch (10a)\",\n",
    "    13: \"13: LCx prox (11)\",\n",
    "    14: \"14: OM1 (12)\",\n",
    "    15: \"15: OM1 branch (12a)\",\n",
    "    16: \"16: LCx mid (13)\",\n",
    "    17: \"17: OM2 (14)\",\n",
    "    18: \"18: OM2 branch (14a)\",\n",
    "    19: \"19: LCx dist (15)\",\n",
    "    20: \"20: PLB (16)\",\n",
    "    21: \"21: PLB branch (16a)\",\n",
    "    22: \"22: PLB branch (16b)\",\n",
    "    23: \"23: PLB branch (16c)\",\n",
    "    24: \"24: OM1 branch (12b)\",\n",
    "    25: \"25: OM2 branch (14b)\",\n",
    "    26: \"26: stenosis\",\n",
    "}\n",
    "\n",
    "categories = [\n",
    "    {\n",
    "        \"id\": class_id,\n",
    "        \"name\": CLASS_ID_TO_LABEL[class_id],\n",
    "        \"supercategory\": \"stenosis\" if class_id == 26 else \"coronary_artery\"\n",
    "    }\n",
    "    for class_id in range(1, 27)\n",
    "]\n",
    "\n",
    "print(f\"‚úì Defined {len(categories)} COCO categories for coronary artery segmentation.\")\n",
    "\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NpEncoder, self).default(obj)\n",
    "\n",
    "def mask_to_rle(mask):\n",
    "    \"\"\"Konwertuje binarnƒÖ maskƒô do formatu RLE zgodnego z COCO.\"\"\"\n",
    "    mask = np.asfortranarray(mask.astype(np.uint8))\n",
    "    rle = mask_utils.encode(mask)\n",
    "    rle['counts'] = rle['counts'].decode('utf-8')\n",
    "    return rle\n",
    "\n",
    "def mask_to_polygon(mask):\n",
    "    \"\"\"Konwertuje binarnƒÖ maskƒô na format polygon COCO.\"\"\"\n",
    "    import cv2\n",
    "    mask_uint8 = mask.astype(np.uint8)\n",
    "    \n",
    "    # Upewnij siƒô, ≈ºe maska jest 2D\n",
    "    if mask_uint8.ndim > 2:\n",
    "        mask_uint8 = mask_uint8.squeeze()\n",
    "    \n",
    "    contours_cv, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    polygons = []\n",
    "    for contour in contours_cv:\n",
    "        if contour.size >= 6:\n",
    "            contour = contour.flatten().tolist()\n",
    "            polygons.append(contour)\n",
    "    return polygons if polygons else [[]]\n",
    "\n",
    "def convert_mask_to_bbox(mask):\n",
    "    \"\"\"Oblicza bounding box z binarnej maski w formacie COCO [x, y, width, height].\"\"\"\n",
    "    y_coords, x_coords = np.where(mask > 0)\n",
    "    \n",
    "    if len(x_coords) == 0 or len(y_coords) == 0:\n",
    "        return [0, 0, 0, 0]\n",
    "    \n",
    "    x_min = int(np.min(x_coords))\n",
    "    y_min = int(np.min(y_coords))\n",
    "    x_max = int(np.max(x_coords))\n",
    "    y_max = int(np.max(y_coords))\n",
    "    \n",
    "    width = x_max - x_min + 1\n",
    "    height = y_max - y_min + 1\n",
    "    \n",
    "    return [x_min, y_min, width, height]\n",
    "\n",
    "def convert_multichannel_to_class_mask(mask):\n",
    "    \"\"\"\n",
    "    Konwertuje maskƒô wielokana≈ÇowƒÖ (H, W, C) na maskƒô z indeksami klas (H, W).\n",
    "    Kana≈Ç 0 = t≈Ço, kana≈Çy 1-26 = klasy (stenosis na kanale 26 lub 27).\n",
    "    \"\"\"\n",
    "    if not isinstance(mask, np.ndarray):\n",
    "        mask = np.array(mask)\n",
    "    \n",
    "    # Je≈õli maska jest ju≈º 2D z indeksami klas, zwr√≥ƒá jƒÖ\n",
    "    if mask.ndim == 2:\n",
    "        return mask\n",
    "    \n",
    "    # Obs≈Çuga maski (H, W, C)\n",
    "    if mask.ndim == 3:\n",
    "        num_channels = mask.shape[2]\n",
    "        \n",
    "        # Sprawd≈∫ czy to maska wielokana≈Çowa (one-hot lub prawdopodobie≈Ñstwa)\n",
    "        if num_channels > 1:\n",
    "            # Pomijamy kana≈Ç 0 (t≈Ço) i bierzemy argmax z pozosta≈Çych\n",
    "            # Kana≈Çy 1-26 odpowiadajƒÖ klasom 1-26\n",
    "            # Je≈õli jest 28 kana≈Ç√≥w: 0=t≈Ço, 1-26=klasy, 27=stenosis(?)\n",
    "            \n",
    "            # Metoda 1: argmax po wszystkich kana≈Çach\n",
    "            class_mask = np.argmax(mask, axis=2)\n",
    "            \n",
    "            # Je≈õli argmax daje 0, to t≈Ço - sprawd≈∫ czy co≈õ jest aktywne\n",
    "            # Dla kana≈Ç√≥w binarnych (0/1), gdzie klasa jest tam gdzie warto≈õƒá > 0\n",
    "            if mask.max() <= 1:\n",
    "                # Maski binarne - znajd≈∫ kt√≥ra klasa jest aktywna dla ka≈ºdego piksela\n",
    "                class_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.uint8)\n",
    "                for ch in range(1, min(num_channels, 27)):  # Kana≈Çy 1-26\n",
    "                    channel_mask = mask[:, :, ch]\n",
    "                    if channel_mask.max() > 0:\n",
    "                        # Binaryzuj kana≈Ç\n",
    "                        binary = (channel_mask > 0.5).astype(np.uint8)\n",
    "                        class_mask[binary > 0] = ch\n",
    "            \n",
    "            return class_mask\n",
    "        else:\n",
    "            # Pojedynczy kana≈Ç\n",
    "            return mask[:, :, 0]\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def extract_instances_from_mask(mask, debug=False):\n",
    "    \"\"\"\n",
    "    Wyodrƒôbnia osobne instancje z maski.\n",
    "    Obs≈Çuguje zar√≥wno maski wielokana≈Çowe (H, W, C) jak i z indeksami klas (H, W).\n",
    "    Zwraca listƒô (class_id, binary_mask) dla ka≈ºdej instancji.\n",
    "    \"\"\"\n",
    "    import cv2\n",
    "    instances = []\n",
    "    \n",
    "    # Konwertuj do numpy\n",
    "    if not isinstance(mask, np.ndarray):\n",
    "        mask = np.array(mask)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"  Input mask shape: {mask.shape}, dtype: {mask.dtype}\")\n",
    "    \n",
    "    # === OBS≈ÅUGA MASKI WIELOKANA≈ÅOWEJ (H, W, C) ===\n",
    "    if mask.ndim == 3 and mask.shape[2] > 1:\n",
    "        num_channels = mask.shape[2]\n",
    "        if debug:\n",
    "            print(f\"  Multi-channel mask detected: {num_channels} channels\")\n",
    "        \n",
    "        # Iteruj po kana≈Çach (1-26 = klasy, 0 = t≈Ço)\n",
    "        for ch in range(1, min(num_channels, 27)):\n",
    "            channel_mask = mask[:, :, ch]\n",
    "            \n",
    "            # Sprawd≈∫ czy kana≈Ç ma jakiekolwiek dane\n",
    "            if channel_mask.max() <= 0:\n",
    "                continue\n",
    "            \n",
    "            # Binaryzuj kana≈Ç\n",
    "            if channel_mask.max() <= 1:\n",
    "                binary_mask = (channel_mask > 0.5).astype(np.uint8)\n",
    "            else:\n",
    "                binary_mask = (channel_mask > 0).astype(np.uint8)\n",
    "            \n",
    "            if np.sum(binary_mask) < 10:\n",
    "                continue\n",
    "            \n",
    "            # Rozdziel na connected components\n",
    "            try:\n",
    "                num_labels, labels = cv2.connectedComponents(binary_mask)\n",
    "                for instance_id in range(1, num_labels):\n",
    "                    instance_mask = (labels == instance_id).astype(np.uint8)\n",
    "                    if np.sum(instance_mask) > 10:\n",
    "                        instances.append((ch, instance_mask))  # ch = class_id\n",
    "            except Exception as e:\n",
    "                if debug:\n",
    "                    print(f\"  Error in connectedComponents for channel {ch}: {e}\")\n",
    "                if np.sum(binary_mask) > 10:\n",
    "                    instances.append((ch, binary_mask))\n",
    "        \n",
    "        return instances\n",
    "    \n",
    "    # === OBS≈ÅUGA MASKI 2D Z INDEKSAMI KLAS ===\n",
    "    if mask.ndim == 3 and mask.shape[2] == 1:\n",
    "        mask = mask[:, :, 0]\n",
    "    \n",
    "    if mask.ndim != 2:\n",
    "        print(f\"  Error: Cannot handle mask with shape {mask.shape}\")\n",
    "        return instances\n",
    "    \n",
    "    if debug:\n",
    "        unique_vals = np.unique(mask)\n",
    "        print(f\"  2D class-index mask: unique values = {unique_vals[:15]}\")\n",
    "    \n",
    "    # Znajd≈∫ wszystkie unikalne klasy (pomijajƒÖc t≈Ço = 0)\n",
    "    unique_classes = np.unique(mask)\n",
    "    unique_classes = unique_classes[unique_classes > 0]\n",
    "    \n",
    "    for class_id in unique_classes:\n",
    "        if class_id > 26:\n",
    "            continue\n",
    "        \n",
    "        class_mask = (mask == class_id).astype(np.uint8)\n",
    "        \n",
    "        if np.sum(class_mask) < 10:\n",
    "            continue\n",
    "        \n",
    "        # Rozdziel na connected components\n",
    "        try:\n",
    "            num_labels, labels = cv2.connectedComponents(class_mask)\n",
    "            for instance_id in range(1, num_labels):\n",
    "                instance_mask = (labels == instance_id).astype(np.uint8)\n",
    "                if np.sum(instance_mask) > 10:\n",
    "                    instances.append((int(class_id), instance_mask))\n",
    "        except Exception as e:\n",
    "            if debug:\n",
    "                print(f\"  Error in connectedComponents for class {class_id}: {e}\")\n",
    "            if np.sum(class_mask) > 10:\n",
    "                instances.append((int(class_id), class_mask))\n",
    "    \n",
    "    return instances\n",
    "\n",
    "def clean_corrupted_cache(mask_dir, img_id):\n",
    "    \"\"\"Usuwa uszkodzony plik cache maski.\"\"\"\n",
    "    import glob\n",
    "    # Szukaj pliku cache dla danego obrazu\n",
    "    cache_pattern = os.path.join(mask_dir, f\"*_{img_id}_*.npz\")\n",
    "    matching_files = glob.glob(cache_pattern)\n",
    "    \n",
    "    # Usu≈Ñ r√≥wnie≈º bezpo≈õredni plik\n",
    "    direct_file = os.path.join(mask_dir, f\"{img_id}.npz\")\n",
    "    if os.path.exists(direct_file):\n",
    "        matching_files.append(direct_file)\n",
    "    \n",
    "    for cache_file in matching_files:\n",
    "        try:\n",
    "            os.remove(cache_file)\n",
    "            print(f\"  üóëÔ∏è  Removed corrupted cache: {cache_file}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è  Failed to remove {cache_file}: {e}\")\n",
    "    \n",
    "    return False\n",
    "\n",
    "# ‚úÖ G≈Å√ìWNA PƒòTLA KONWERSJI\n",
    "for split_name in [\"train\", \"valid\", \"test\"]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing {split_name.upper()} split...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    arcade_image_set = split_name if split_name != \"valid\" else \"val\"\n",
    "    \n",
    "    # Za≈Çaduj dataset ARCADE\n",
    "    try:\n",
    "        current_dataset = ARCADESemanticSegmentation(\n",
    "            \"dataset/\",\n",
    "            image_set=arcade_image_set,\n",
    "            download=False\n",
    "        )\n",
    "        print(\"‚úì Using ARCADESemanticSegmentation dataset\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load ARCADESemanticSegmentation: {e}\")\n",
    "        print(\"Trying ARCADEBinarySegmentation...\")\n",
    "        from torch_arcade import ARCADEBinarySegmentation\n",
    "        current_dataset = ARCADEBinarySegmentation(\n",
    "            \"dataset/\",\n",
    "            image_set=arcade_image_set,\n",
    "            download=False\n",
    "        )\n",
    "        print(\"‚úì Using ARCADEBinarySegmentation dataset\")\n",
    "    \n",
    "    coco_images = []\n",
    "    coco_annotations = []\n",
    "    annotation_id_counter = 0\n",
    "    image_id_counter = 0\n",
    "    images_without_objects = 0\n",
    "    total_instances = 0\n",
    "    skipped_images = 0\n",
    "    \n",
    "    # Statystyki klas\n",
    "    class_instance_counts = {i: 0 for i in range(1, 27)}\n",
    "    \n",
    "    # Iteruj przez dataset z obs≈ÇugƒÖ b≈Çƒôd√≥w\n",
    "    dataset_length = len(current_dataset)\n",
    "    i = 0\n",
    "    \n",
    "    while i < dataset_length:\n",
    "        try:\n",
    "            img, mask = current_dataset[i]\n",
    "            \n",
    "            # Diagnostyka pierwszego obrazu\n",
    "            if i == 0:\n",
    "                mask_np = np.array(mask) if not isinstance(mask, np.ndarray) else mask\n",
    "                print(f\"\\nüîç First sample diagnostics:\")\n",
    "                print(f\"   Image type: {type(img)}, size: {img.size if hasattr(img, 'size') else 'N/A'}\")\n",
    "                print(f\"   Mask shape: {mask_np.shape}, dtype: {mask_np.dtype}\")\n",
    "                print(f\"   Mask min: {mask_np.min()}, max: {mask_np.max()}\")\n",
    "                if mask_np.ndim == 3:\n",
    "                    print(f\"   Mask channels with data: \", end=\"\")\n",
    "                    active_channels = [ch for ch in range(mask_np.shape[2]) if mask_np[:,:,ch].max() > 0]\n",
    "                    print(f\"{active_channels[:10]}{'...' if len(active_channels) > 10 else ''}\")\n",
    "            \n",
    "            image_id_counter += 1\n",
    "            img_width, img_height = img.size\n",
    "            image_filename = f\"{image_id_counter:06d}.jpg\"\n",
    "            image_path = os.path.join(output_root_dir, split_name, image_filename)\n",
    "            \n",
    "            # Zapisz obraz\n",
    "            img.save(image_path)\n",
    "            \n",
    "            coco_images.append({\n",
    "                \"id\": image_id_counter,\n",
    "                \"width\": img_width,\n",
    "                \"height\": img_height,\n",
    "                \"file_name\": image_filename\n",
    "            })\n",
    "            \n",
    "            # Wyodrƒôbnij wszystkie instancje\n",
    "            instances = extract_instances_from_mask(mask, debug=(i == 0))\n",
    "            \n",
    "            if len(instances) == 0:\n",
    "                images_without_objects += 1\n",
    "            \n",
    "            # Dla ka≈ºdej instancji, stw√≥rz osobnƒÖ annotacjƒô\n",
    "            for class_id, instance_mask in instances:\n",
    "                bbox = convert_mask_to_bbox(instance_mask)\n",
    "                area = int(np.sum(instance_mask))\n",
    "                \n",
    "                if area > 0 and bbox[2] > 0 and bbox[3] > 0:\n",
    "                    try:\n",
    "                        segmentation = mask_to_polygon(instance_mask)\n",
    "                        \n",
    "                        # Sprawd≈∫ czy polygon jest prawid≈Çowy\n",
    "                        if segmentation and segmentation != [[]] and len(segmentation[0]) >= 6:\n",
    "                            annotation_id_counter += 1\n",
    "                            total_instances += 1\n",
    "                            class_instance_counts[class_id] += 1\n",
    "                            \n",
    "                            coco_annotations.append({\n",
    "                                \"id\": annotation_id_counter,\n",
    "                                \"image_id\": image_id_counter,\n",
    "                                \"category_id\": class_id,\n",
    "                                \"segmentation\": segmentation,\n",
    "                                \"area\": area,\n",
    "                                \"bbox\": bbox,\n",
    "                                \"iscrowd\": 0\n",
    "                            })\n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ö†Ô∏è  Warning: Failed to encode mask for image {image_id_counter}, \"\n",
    "                              f\"class {class_id}: {e}\")\n",
    "            \n",
    "            # Progress\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"  Processed {i + 1}/{dataset_length} images... ({total_instances} instances, {skipped_images} skipped)\")\n",
    "            \n",
    "            i += 1\n",
    "            \n",
    "        except (BadZipFile, OSError, IOError) as e:\n",
    "            print(f\"\\n‚ö†Ô∏è  Error loading image {i}: {e}\")\n",
    "            print(f\"  Attempting to clean corrupted cache...\")\n",
    "            \n",
    "            # Spr√≥buj usunƒÖƒá uszkodzony cache\n",
    "            if hasattr(current_dataset, 'mask_dir'):\n",
    "                try:\n",
    "                    # Pobierz ID obrazu\n",
    "                    img_filename = current_dataset.ids[i]\n",
    "                    img_id = current_dataset.file_to_id.get(img_filename, i)\n",
    "                    clean_corrupted_cache(current_dataset.mask_dir, img_id)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            skipped_images += 1\n",
    "            i += 1\n",
    "            continue\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Unexpected error at image {i}: {type(e).__name__}: {e}\")\n",
    "            skipped_images += 1\n",
    "            i += 1\n",
    "            continue\n",
    "    \n",
    "    # Zapisz COCO JSON\n",
    "    coco_json = {\n",
    "        \"images\": coco_images,\n",
    "        \"annotations\": coco_annotations,\n",
    "        \"categories\": categories\n",
    "    }\n",
    "    \n",
    "    output_annotation_path = os.path.join(output_root_dir, split_name, \"_annotations.coco.json\")\n",
    "    \n",
    "    with open(output_annotation_path, \"w\") as f:\n",
    "        json.dump(coco_json, f, indent=4, cls=NpEncoder)\n",
    "    \n",
    "    # Statystyki\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úÖ {split_name.upper()} SPLIT COMPLETE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  üìä Images: {len(coco_images)}\")\n",
    "    print(f\"  üìä Annotations: {len(coco_annotations)}\")\n",
    "    print(f\"  üìä Total instances: {total_instances}\")\n",
    "    print(f\"  üìä Images without objects: {images_without_objects}\")\n",
    "    print(f\"  üìä Skipped (corrupted): {skipped_images}\")\n",
    "    print(f\"\\n  ü´Ä Class distribution (top 15):\")\n",
    "    \n",
    "    sorted_classes = sorted(class_instance_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    for class_id, count in sorted_classes[:15]:\n",
    "        if count > 0:\n",
    "            print(f\"     {CLASS_ID_TO_FULL_LABEL[class_id]:30s}: {count:5d} instances\")\n",
    "    \n",
    "    remaining = sum(count for _, count in sorted_classes[15:])\n",
    "    if remaining > 0:\n",
    "        active_classes = len([c for c in sorted_classes[15:] if c[1] > 0])\n",
    "        print(f\"     ... and {remaining} instances in {active_classes} other classes\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"‚úÖ COCO-formatted multi-class CORONARY ARTERY SEGMENTATION dataset complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"üìÅ Output directory: {output_root_dir}\")\n",
    "print(f\"ü´Ä 26 classes: 25 coronary arteries + 1 stenosis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KnnT4EVVB0WV",
    "outputId": "14e5407c-6170-459c-a0eb-12addce6a8c2"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from rfdetr import RFDETRSegLarge\n",
    "\n",
    "# ZA≈ÅADUJ PRETRAINED MODEL\n",
    "model = RFDETRSegLarge(\n",
    "    load_pretrain=True,  # Za≈Çaduj wagi pretrained\n",
    "    pretrained_weights=\"rf-detr-seg-large.pt\"  # Domy≈õlna warto≈õƒá\n",
    ")\n",
    "\n",
    "model.train(\n",
    "    dataset_dir=\"./arcade_coco_dataset_seg\",\n",
    "    epochs=8,\n",
    "    batch_size=1,\n",
    "    grad_accum_steps=16,\n",
    "    lr=1e-4,\n",
    "    output_dir=\"output_segmentation\"\n",
    ")\n",
    "\n",
    "model.optimize_for_inference()\n",
    "\n",
    "print(\"\\n Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_gpu_memory(model, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xT1tPwZS_-6t"
   },
   "source": [
    "## Run Inference with Fine-tuned RF-DETR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "id": "msor_5HgAkm3",
    "outputId": "89dbdd3c-627e-41ad-bf8d-8f90a821b130"
   },
   "outputs": [],
   "source": [
    "from rfdetr import RFDETRSegLarge\n",
    "import supervision as sv\n",
    "from PIL import Image\n",
    "\n",
    "image = Image.open(\"/home/ives/rafal/rf-detr-seg/notebooks/notebooks/wum/4_I0515980.VIM.DCM.21.png\").convert(\"RGB\")\n",
    "\n",
    "detections = model.predict(image, threshold=0.5)\n",
    "\n",
    "# Klasy z modelu\n",
    "model_classes = model.class_names  # {1: \"class_name\", ...}\n",
    "\n",
    "# ‚îÄ‚îÄ Parametry wizualizacji (l≈ºejsze) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "text_scale = sv.calculate_optimal_text_scale(\n",
    "    resolution_wh=image.size\n",
    ") * 0.6   # üîΩ mniejszy tekst\n",
    "\n",
    "thickness = sv.calculate_optimal_line_thickness(\n",
    "    resolution_wh=image.size\n",
    ") * 0.6   # üîΩ cie≈Ñsze obramowania\n",
    "\n",
    "palette = sv.ColorPalette.from_hex([\n",
    "    \"#ffff00\", \"#ff9b00\", \"#ff66ff\",\n",
    "    \"#3399ff\", \"#ff66b2\", \"#ff8080\"\n",
    "])\n",
    "\n",
    "# Maski ‚Äì bardziej przezroczyste\n",
    "mask_annotator = sv.MaskAnnotator(\n",
    "    color=palette,\n",
    "    opacity=0.3   # üîΩ wiƒôksza przezroczysto≈õƒá\n",
    ")\n",
    "\n",
    "# Etykiety ‚Äì ma≈Çe i dyskretne\n",
    "label_annotator = sv.LabelAnnotator(\n",
    "    color=palette,\n",
    "    text_color=sv.Color.BLACK,\n",
    "    text_scale=text_scale,\n",
    "    text_thickness=1\n",
    ")\n",
    "\n",
    "# Etykiety predykcji\n",
    "detections_labels = [\n",
    "    f\"{model_classes.get(class_id + 1, f'class_{class_id}')} {confidence:.2f}\"\n",
    "    for class_id, confidence in zip(detections.class_id, detections.confidence)\n",
    "]\n",
    "\n",
    "# ‚îÄ‚îÄ Wizualizacja ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "result_image = image.copy()\n",
    "result_image = mask_annotator.annotate(result_image, detections)\n",
    "result_image = label_annotator.annotate(\n",
    "    result_image,\n",
    "    detections,\n",
    "    detections_labels\n",
    ")\n",
    "\n",
    "sv.plot_image(\n",
    "    image=result_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516
    },
    "id": "Jf0v2Bv6T7wA",
    "outputId": "69cda435-c754-4a33-9cb7-b782cbc27e9a"
   },
   "outputs": [],
   "source": [
    "from rfdetr import RFDETRSegLarge\n",
    "import supervision as sv\n",
    "from PIL import Image\n",
    "\n",
    "path, image, annotations = ds[50]\n",
    "image = Image.open(path).convert('RGB')\n",
    "\n",
    "detections = model.predict(image, threshold=0.5)\n",
    "\n",
    "# üî• POBIERZ NAZWY KLAS Z MODELU (nie z datasetu)\n",
    "model_classes = model.class_names  # To zwraca dict: {1: 'class_name', ...}\n",
    "\n",
    "print(f\"‚úì Model classes: {model_classes}\")\n",
    "print(f\"‚úì Dataset classes: {ds.classes}\")\n",
    "\n",
    "# Parametry wizualizacji\n",
    "text_scale = sv.calculate_optimal_text_scale(resolution_wh=image.size)\n",
    "thickness = sv.calculate_optimal_line_thickness(resolution_wh=image.size)\n",
    "color = sv.ColorPalette.from_hex([\n",
    "    \"#ffff00\", \"#ff9b00\", \"#ff66ff\", \"#3399ff\", \"#ff66b2\", \"#ff8080\"\n",
    "])\n",
    "\n",
    "# Annotatory dla segmentacji\n",
    "mask_annotator = sv.MaskAnnotator(color=color, opacity=0.5)\n",
    "label_annotator = sv.LabelAnnotator(\n",
    "    color=color,\n",
    "    text_color=sv.Color.BLACK,\n",
    "    text_scale=text_scale\n",
    ")\n",
    "\n",
    "# ‚úÖ U≈ªYJ NAZW KLAS Z MODELU\n",
    "annotations_labels = [\n",
    "    model_classes.get(class_id + 1, ds.classes[class_id] if class_id < len(ds.classes) else f\"class_{class_id}\")\n",
    "    for class_id in annotations.class_id\n",
    "]\n",
    "\n",
    "detections_labels = [\n",
    "    f\"{model_classes.get(class_id + 1, f'class_{class_id}')} {confidence:.2f}\"\n",
    "    for class_id, confidence in zip(detections.class_id, detections.confidence)\n",
    "]\n",
    "\n",
    "# Wizualizacja\n",
    "annotation_image = image.copy()\n",
    "annotation_image = mask_annotator.annotate(annotation_image, annotations)\n",
    "annotation_image = label_annotator.annotate(annotation_image, annotations, annotations_labels)\n",
    "\n",
    "detections_image = image.copy()\n",
    "detections_image = mask_annotator.annotate(detections_image, detections)\n",
    "detections_image = label_annotator.annotate(detections_image, detections, detections_labels)\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=[annotation_image, detections_image],\n",
    "    grid_size=(1, 2),\n",
    "    titles=[\"Ground Truth\", \"Prediction\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üé® WIZUALIZACJA Z MASKAMI - 28 KLAS Z R√ì≈ªNYMI KOLORAMI\n",
    "from rfdetr import RFDETRSegLarge\n",
    "import supervision as sv\n",
    "from PIL import Image\n",
    "\n",
    "# ds = sv.DetectionDataset.from_coco(\n",
    "#     images_directory_path=f\"./arcade_coco_dataset_seg/test\",\n",
    "#     annotations_path=f\"./arcade_coco_dataset_seg/test/_annotations.coco.json\",\n",
    "# )\n",
    "\n",
    "# Wybierz pr√≥bkƒô\n",
    "sample_idx = 4\n",
    "path, _, annotations = ds[sample_idx]\n",
    "image = Image.open(path).convert('RGB')\n",
    "\n",
    "detections = model.predict(image, threshold=0.5)\n",
    "\n",
    "# üîç DIAGNOSTYKA\n",
    "print(f\"‚úì Annotations has mask: {annotations.mask is not None}\")\n",
    "print(f\"‚úì Detections has mask: {detections.mask is not None}\")\n",
    "print(f\"‚úì GT class_ids: {annotations.class_id}\")\n",
    "print(f\"‚úì Pred class_ids: {detections.class_id}\")\n",
    "\n",
    "# üî• POBIERZ NAZWY KLAS Z MODELU\n",
    "model_classes = model.class_names  # Dict: {1: 'class_name', ...}\n",
    "print(f\"‚úì Model classes: {model_classes}\")\n",
    "print(f\"‚úì Dataset classes ({len(ds.classes)}): {ds.classes}\")\n",
    "\n",
    "# Parametry wizualizacji\n",
    "text_scale = sv.calculate_optimal_text_scale(resolution_wh=image.size)\n",
    "thickness = sv.calculate_optimal_line_thickness(resolution_wh=image.size)\n",
    "\n",
    "# ‚úÖ PALETA 26 KOLOR√ìW - r√≥≈ºne odcienie dla ka≈ºdej klasy tƒôtnic wie≈Ñcowych\n",
    "COLOR_PALETTE_26 = sv.ColorPalette.from_hex([\n",
    "    \"#FF0000\", \"#00FF00\", \"#0000FF\", \"#FFFF00\", \"#FF00FF\", \"#00FFFF\", \"#FF8000\",  # 0-6\n",
    "    \"#8000FF\", \"#00FF80\", \"#FF0080\", \"#80FF00\", \"#0080FF\", \"#FF8080\", \"#80FF80\",  # 7-13\n",
    "    \"#8080FF\", \"#FFFF80\", \"#FF80FF\", \"#80FFFF\", \"#C00000\", \"#00C000\", \"#0000C0\",  # 14-20\n",
    "    \"#C0C000\", \"#C000C0\", \"#00C0C0\", \"#FFA500\", \"#A500FF\"                          # 21-25\n",
    "])\n",
    "\n",
    "# ‚úÖ Annotatory z paletƒÖ kolor√≥w + COLOR_LOOKUP.CLASS (kolor na podstawie class_id!)\n",
    "mask_annotator = sv.MaskAnnotator(\n",
    "    color=COLOR_PALETTE_26, \n",
    "    opacity=0.5,\n",
    "    color_lookup=sv.ColorLookup.CLASS  # üîë KLUCZOWE: kolor na podstawie class_id\n",
    ")\n",
    "label_annotator = sv.LabelAnnotator(\n",
    "    color=COLOR_PALETTE_26,\n",
    "    text_color=sv.Color.BLACK,\n",
    "    text_scale=text_scale,\n",
    "    color_lookup=sv.ColorLookup.CLASS  # üîë KLUCZOWE: kolor na podstawie class_id\n",
    ")\n",
    "\n",
    "# ‚úÖ U≈ªYJ NAZW KLAS Z MODELU (mapowanie class_id -> nazwa)\n",
    "# Teraz zar√≥wno GT jak i model u≈ºywajƒÖ 1-indexed class_id (1-26)\n",
    "annotations_labels = [\n",
    "    model_classes.get(class_id, f\"class_{class_id}\")\n",
    "    for class_id in annotations.class_id\n",
    "]\n",
    "\n",
    "detections_labels = [\n",
    "    f\"{model_classes.get(class_id, f'class_{class_id}')} {confidence:.2f}\"\n",
    "    for class_id, confidence in zip(detections.class_id, detections.confidence)\n",
    "]\n",
    "\n",
    "# üé® WIZUALIZACJA GROUND TRUTH\n",
    "annotation_image = image.copy()\n",
    "if annotations.mask is not None:\n",
    "    annotation_image = mask_annotator.annotate(annotation_image, annotations)\n",
    "annotation_image = label_annotator.annotate(annotation_image, annotations, annotations_labels)\n",
    "\n",
    "# üé® WIZUALIZACJA PREDIKCJI\n",
    "detections_image = image.copy()\n",
    "if detections.mask is not None:\n",
    "    detections_image = mask_annotator.annotate(detections_image, detections)\n",
    "detections_image = label_annotator.annotate(detections_image, detections, detections_labels)\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=[annotation_image, detections_image],\n",
    "    grid_size=(1, 2),\n",
    "    titles=[\"Ground Truth\", \"Prediction\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from \n",
    "targets = []\n",
    "predictions = []\n",
    "\n",
    "for path, image, annotations in tqdm(ds):\n",
    "    # Za≈Çaduj obraz\n",
    "    image = Image.open(path)\n",
    "\n",
    "    # ‚úÖ KONWERTUJ DO RGB (nawet je≈õli ju≈º jest RGB, to nie szkodzi)\n",
    "    image = image.convert('RGB')\n",
    "\n",
    "    # Predykcja\n",
    "    detections = model.predict(image, threshold=0)\n",
    "\n",
    "    targets.append(annotations)\n",
    "    predictions.append(detections)\n",
    "\n",
    "map_metric = MeanAveragePrecision()\n",
    "map_result = map_metric.update(predictions, targets).compute()\n",
    "print(map_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Ewaluacja Segmentacji - Dice/IoU na zbiorze testowym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import supervision as sv\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SegmentationMetrics:\n",
    "    \"\"\"Container for segmentation evaluation metrics.\"\"\"\n",
    "    dice_scores: List[float] = field(default_factory=list)\n",
    "    iou_scores: List[float] = field(default_factory=list)\n",
    "    per_class_dice: Dict[int, List[float]] = field(default_factory=lambda: defaultdict(list))\n",
    "    per_class_iou: Dict[int, List[float]] = field(default_factory=lambda: defaultdict(list))\n",
    "    \n",
    "    @property\n",
    "    def mean_dice(self) -> float:\n",
    "        return np.mean(self.dice_scores) if self.dice_scores else 0.0\n",
    "    \n",
    "    @property\n",
    "    def mean_iou(self) -> float:\n",
    "        return np.mean(self.iou_scores) if self.iou_scores else 0.0\n",
    "    \n",
    "    @property\n",
    "    def std_dice(self) -> float:\n",
    "        return np.std(self.dice_scores) if self.dice_scores else 0.0\n",
    "    \n",
    "    @property\n",
    "    def std_iou(self) -> float:\n",
    "        return np.std(self.iou_scores) if self.iou_scores else 0.0\n",
    "\n",
    "\n",
    "def compute_mask_iou(mask1: np.ndarray, mask2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute IoU (Intersection over Union) between two binary masks.\n",
    "    \n",
    "    Args:\n",
    "        mask1: First binary mask (H, W) or (N, H, W)\n",
    "        mask2: Second binary mask (H, W) or (N, H, W)\n",
    "    \n",
    "    Returns:\n",
    "        IoU score [0, 1]\n",
    "    \"\"\"\n",
    "    mask1 = mask1.astype(bool)\n",
    "    mask2 = mask2.astype(bool)\n",
    "    \n",
    "    intersection = np.logical_and(mask1, mask2).sum()\n",
    "    union = np.logical_or(mask1, mask2).sum()\n",
    "    \n",
    "    if union == 0:\n",
    "        return 1.0 if intersection == 0 else 0.0\n",
    "    \n",
    "    return float(intersection / union)\n",
    "\n",
    "\n",
    "def compute_mask_dice(mask1: np.ndarray, mask2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute Dice coefficient between two binary masks.\n",
    "    \n",
    "    Dice = 2 * |A ‚à© B| / (|A| + |B|)\n",
    "    \n",
    "    Args:\n",
    "        mask1: First binary mask (H, W) or (N, H, W)\n",
    "        mask2: Second binary mask (H, W) or (N, H, W)\n",
    "    \n",
    "    Returns:\n",
    "        Dice score [0, 1]\n",
    "    \"\"\"\n",
    "    mask1 = mask1.astype(bool)\n",
    "    mask2 = mask2.astype(bool)\n",
    "    \n",
    "    intersection = np.logical_and(mask1, mask2).sum()\n",
    "    sum_masks = mask1.sum() + mask2.sum()\n",
    "    \n",
    "    if sum_masks == 0:\n",
    "        return 1.0 if intersection == 0 else 0.0\n",
    "    \n",
    "    return float(2.0 * intersection / sum_masks)\n",
    "\n",
    "\n",
    "def merge_masks_by_class(\n",
    "    masks: np.ndarray, \n",
    "    class_ids: np.ndarray, \n",
    "    num_classes: int = 26\n",
    ") -> Dict[int, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Merge instance masks into per-class semantic masks.\n",
    "    \n",
    "    Args:\n",
    "        masks: Instance masks (N, H, W)\n",
    "        class_ids: Class ID for each mask (N,)\n",
    "        num_classes: Total number of classes\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping class_id -> merged binary mask\n",
    "    \"\"\"\n",
    "    if masks is None or len(masks) == 0:\n",
    "        return {}\n",
    "    \n",
    "    h, w = masks.shape[1], masks.shape[2]\n",
    "    class_masks = {}\n",
    "    \n",
    "    for class_id in np.unique(class_ids):\n",
    "        class_mask = np.zeros((h, w), dtype=bool)\n",
    "        for mask, cid in zip(masks, class_ids):\n",
    "            if cid == class_id:\n",
    "                class_mask = np.logical_or(class_mask, mask)\n",
    "        class_masks[int(class_id)] = class_mask\n",
    "    \n",
    "    return class_masks\n",
    "\n",
    "\n",
    "def evaluate_segmentation_on_dataset(\n",
    "    model,\n",
    "    dataset: sv.DetectionDataset,\n",
    "    threshold: float = 0.5,\n",
    "    iou_matching_threshold: float = 0.5,\n",
    "    num_classes: int = 26,\n",
    "    verbose: bool = True,\n",
    ") -> SegmentationMetrics:\n",
    "    \"\"\"\n",
    "    Evaluate segmentation model on entire dataset.\n",
    "    \n",
    "    Computes Dice and IoU metrics:\n",
    "    1. Per-image global mask (all instances merged)\n",
    "    2. Per-class masks (instances of same class merged)\n",
    "    \n",
    "    Args:\n",
    "        model: RF-DETR model with predict() method\n",
    "        dataset: Supervision DetectionDataset with masks\n",
    "        threshold: Detection confidence threshold\n",
    "        iou_matching_threshold: IoU threshold for matching GT to predictions\n",
    "        num_classes: Number of classes in dataset\n",
    "        verbose: Print progress\n",
    "    \n",
    "    Returns:\n",
    "        SegmentationMetrics with all computed scores\n",
    "    \"\"\"\n",
    "    metrics = SegmentationMetrics()\n",
    "    \n",
    "    # Get class names from model if available\n",
    "    class_names = getattr(model, 'class_names', {})\n",
    "    \n",
    "    iterator = tqdm(range(len(dataset)), desc=\"Evaluating\") if verbose else range(len(dataset))\n",
    "    \n",
    "    for idx in iterator:\n",
    "        path, _, gt_annotations = dataset[idx]\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        \n",
    "        # Get predictions\n",
    "        pred_detections = model.predict(image, threshold=threshold)\n",
    "        \n",
    "        # Check if masks are available\n",
    "        gt_masks = gt_annotations.mask\n",
    "        pred_masks = pred_detections.mask\n",
    "        \n",
    "        if gt_masks is None:\n",
    "            if verbose:\n",
    "                print(f\"‚ö†Ô∏è Image {idx}: No GT masks, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Get image dimensions\n",
    "        h, w = gt_masks.shape[1], gt_masks.shape[2] if len(gt_masks) > 0 else image.size[::-1]\n",
    "        \n",
    "        # === 1. GLOBAL MASK EVALUATION ===\n",
    "        # Merge all GT masks into one\n",
    "        gt_global = np.zeros((h, w), dtype=bool)\n",
    "        for mask in gt_masks:\n",
    "            gt_global = np.logical_or(gt_global, mask)\n",
    "        \n",
    "        # Merge all prediction masks into one\n",
    "        pred_global = np.zeros((h, w), dtype=bool)\n",
    "        if pred_masks is not None and len(pred_masks) > 0:\n",
    "            for mask in pred_masks:\n",
    "                pred_global = np.logical_or(pred_global, mask)\n",
    "        \n",
    "        # Compute global metrics\n",
    "        global_dice = compute_mask_dice(gt_global, pred_global)\n",
    "        global_iou = compute_mask_iou(gt_global, pred_global)\n",
    "        \n",
    "        metrics.dice_scores.append(global_dice)\n",
    "        metrics.iou_scores.append(global_iou)\n",
    "        \n",
    "        # === 2. PER-CLASS EVALUATION ===\n",
    "        gt_class_masks = merge_masks_by_class(gt_masks, gt_annotations.class_id, num_classes)\n",
    "        \n",
    "        if pred_masks is not None and len(pred_masks) > 0:\n",
    "            pred_class_masks = merge_masks_by_class(pred_masks, pred_detections.class_id, num_classes)\n",
    "        else:\n",
    "            pred_class_masks = {}\n",
    "        \n",
    "        # Compute per-class metrics\n",
    "        all_classes = set(gt_class_masks.keys()) | set(pred_class_masks.keys())\n",
    "        \n",
    "        for class_id in all_classes:\n",
    "            gt_mask = gt_class_masks.get(class_id, np.zeros((h, w), dtype=bool))\n",
    "            pred_mask = pred_class_masks.get(class_id, np.zeros((h, w), dtype=bool))\n",
    "            \n",
    "            class_dice = compute_mask_dice(gt_mask, pred_mask)\n",
    "            class_iou = compute_mask_iou(gt_mask, pred_mask)\n",
    "            \n",
    "            metrics.per_class_dice[class_id].append(class_dice)\n",
    "            metrics.per_class_iou[class_id].append(class_iou)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_evaluation_report(\n",
    "    metrics: SegmentationMetrics,\n",
    "    class_names: Optional[Dict[int, str]] = None,\n",
    "    show_per_class: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Print formatted evaluation report.\n",
    "    \n",
    "    Args:\n",
    "        metrics: Computed SegmentationMetrics\n",
    "        class_names: Optional mapping class_id -> name\n",
    "        show_per_class: Whether to show per-class breakdown\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìä SEGMENTATION EVALUATION REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nüéØ GLOBAL METRICS (all classes merged per image)\")\n",
    "    print(f\"   Images evaluated: {len(metrics.dice_scores)}\")\n",
    "    print(f\"   Mean Dice:  {metrics.mean_dice:.4f} ¬± {metrics.std_dice:.4f}\")\n",
    "    print(f\"   Mean IoU:   {metrics.mean_iou:.4f} ¬± {metrics.std_iou:.4f}\")\n",
    "    \n",
    "    if show_per_class and metrics.per_class_dice:\n",
    "        print(f\"\\nüìà PER-CLASS METRICS\")\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"{'Class':<30} {'Dice':>10} {'IoU':>10} {'Samples':>10}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Sort by class_id\n",
    "        sorted_classes = sorted(metrics.per_class_dice.keys())\n",
    "        \n",
    "        class_dice_means = []\n",
    "        class_iou_means = []\n",
    "        \n",
    "        for class_id in sorted_classes:\n",
    "            dice_scores = metrics.per_class_dice[class_id]\n",
    "            iou_scores = metrics.per_class_iou[class_id]\n",
    "            \n",
    "            mean_dice = np.mean(dice_scores)\n",
    "            mean_iou = np.mean(iou_scores)\n",
    "            n_samples = len(dice_scores)\n",
    "            \n",
    "            class_dice_means.append(mean_dice)\n",
    "            class_iou_means.append(mean_iou)\n",
    "            \n",
    "            # Get class name\n",
    "            if class_names and class_id in class_names:\n",
    "                name = f\"{class_id}: {class_names[class_id]}\"\n",
    "            else:\n",
    "                name = f\"Class {class_id}\"\n",
    "            \n",
    "            print(f\"{name:<30} {mean_dice:>10.4f} {mean_iou:>10.4f} {n_samples:>10}\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "        print(f\"{'MACRO AVERAGE':<30} {np.mean(class_dice_means):>10.4f} {np.mean(class_iou_means):>10.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "def plot_per_class_metrics(\n",
    "    metrics: SegmentationMetrics,\n",
    "    class_names: Optional[Dict[int, str]] = None,\n",
    "    figsize: Tuple[int, int] = (14, 6),\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot per-class Dice and IoU scores as bar charts.\n",
    "    \n",
    "    Args:\n",
    "        metrics: Computed SegmentationMetrics\n",
    "        class_names: Optional mapping class_id -> name\n",
    "        figsize: Figure size\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    sorted_classes = sorted(metrics.per_class_dice.keys())\n",
    "    \n",
    "    dice_means = [np.mean(metrics.per_class_dice[c]) for c in sorted_classes]\n",
    "    iou_means = [np.mean(metrics.per_class_iou[c]) for c in sorted_classes]\n",
    "    \n",
    "    # Get class labels\n",
    "    if class_names:\n",
    "        labels = [class_names.get(c, f\"Class {c}\") for c in sorted_classes]\n",
    "    else:\n",
    "        labels = [f\"Class {c}\" for c in sorted_classes]\n",
    "    \n",
    "    x = np.arange(len(sorted_classes))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    bars1 = ax.bar(x - width/2, dice_means, width, label='Dice', color='steelblue')\n",
    "    bars2 = ax.bar(x + width/2, iou_means, width, label='IoU', color='coral')\n",
    "    \n",
    "    ax.set_xlabel('Class')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Per-Class Segmentation Metrics (Dice & IoU)')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1.0)\n",
    "    ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.axhline(y=np.mean(dice_means), color='steelblue', linestyle=':', alpha=0.7, label=f'Mean Dice={np.mean(dice_means):.3f}')\n",
    "    ax.axhline(y=np.mean(iou_means), color='coral', linestyle=':', alpha=0.7, label=f'Mean IoU={np.mean(iou_means):.3f}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    print(f\"   Macro Dice: {np.mean(dice_means):.4f}\")\n",
    "    print(f\"   Macro IoU:  {np.mean(iou_means):.4f}\")\n",
    "    print(f\"   Best class (Dice): {labels[np.argmax(dice_means)]} = {max(dice_means):.4f}\")\n",
    "    print(f\"   Worst class (Dice): {labels[np.argmin(dice_means)]} = {min(dice_means):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ URUCHOM EWALUACJƒò NA ZBIORZE TESTOWYM\n",
    "# Upewnij siƒô, ≈ºe model i ds (dataset) sƒÖ za≈Çadowane\n",
    "\n",
    "# Je≈õli nie masz jeszcze za≈Çadowanego datasetu:\n",
    "# ds = sv.DetectionDataset.from_coco(\n",
    "#     images_directory_path=\"./arcade_coco_dataset_seg/test\",\n",
    "#     annotations_path=\"./arcade_coco_dataset_seg/test/_annotations.coco.json\",\n",
    "# )\n",
    "\n",
    "# Uruchom ewaluacjƒô\n",
    "metrics = evaluate_segmentation_on_dataset(\n",
    "    model=model,\n",
    "    dataset=ds,\n",
    "    threshold=0.5,\n",
    "    num_classes=26,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Wy≈õwietl raport\n",
    "print_evaluation_report(\n",
    "    metrics=metrics,\n",
    "    class_names=model.class_names if hasattr(model, 'class_names') else None,\n",
    "    show_per_class=True\n",
    ")\n",
    "\n",
    "# Wizualizacja per-class\n",
    "plot_per_class_metrics(\n",
    "    metrics=metrics,\n",
    "    class_names=model.class_names if hasattr(model, 'class_names') else None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Analiza maxDets dla ARCADE Dataset\n",
    "\n",
    "Parametr `maxDets` w pycocotools okre≈õla maksymalnƒÖ liczbƒô detekcji na obraz branych pod uwagƒô przy ewaluacji.\n",
    "\n",
    "**Domy≈õlne warto≈õci:** `[1, 10, 100]`\n",
    "\n",
    "**Problem dla ARCADE:**\n",
    "- Dataset ma 25-26 klas segment√≥w naczy≈Ñ wie≈Ñcowych\n",
    "- Na jednym obrazie mo≈ºe byƒá 10-40+ instancji (segmenty tƒôtnic)\n",
    "- Je≈õli `maxDets < liczba_obiekt√≥w`, czƒô≈õƒá detekcji jest ignorowana ‚Üí zani≈ºone metryki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä ANALIZA ROZK≈ÅADU LICZBY OBIEKT√ìW PER OBRAZ W ARCADE\n",
    "# Sprawd≈∫ ile obiekt√≥w jest na obrazach, ≈ºeby dobraƒá odpowiedni maxDets\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_objects_per_image(coco_json_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze distribution of objects per image in COCO dataset.\n",
    "    \n",
    "    Returns dict with statistics useful for setting maxDets parameter.\n",
    "    \"\"\"\n",
    "    coco = COCO(coco_json_path)\n",
    "    \n",
    "    # Count objects per image\n",
    "    objects_per_image = []\n",
    "    for img_id in coco.getImgIds():\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        objects_per_image.append(len(ann_ids))\n",
    "    \n",
    "    objects_per_image = np.array(objects_per_image)\n",
    "    \n",
    "    stats = {\n",
    "        \"num_images\": len(objects_per_image),\n",
    "        \"total_objects\": int(objects_per_image.sum()),\n",
    "        \"min\": int(objects_per_image.min()),\n",
    "        \"max\": int(objects_per_image.max()),\n",
    "        \"mean\": float(objects_per_image.mean()),\n",
    "        \"median\": float(np.median(objects_per_image)),\n",
    "        \"std\": float(objects_per_image.std()),\n",
    "        \"p90\": float(np.percentile(objects_per_image, 90)),\n",
    "        \"p95\": float(np.percentile(objects_per_image, 95)),\n",
    "        \"p99\": float(np.percentile(objects_per_image, 99)),\n",
    "    }\n",
    "    \n",
    "    # Suggested maxDets values\n",
    "    stats[\"suggested_maxDets\"] = [\n",
    "        1, \n",
    "        10, \n",
    "        int(np.ceil(stats[\"p99\"] / 10) * 10),  # Round up to nearest 10\n",
    "        max(100, int(np.ceil(stats[\"max\"] / 50) * 50))  # At least 100, round to 50\n",
    "    ]\n",
    "    \n",
    "    return stats, objects_per_image\n",
    "\n",
    "\n",
    "def plot_objects_distribution(objects_per_image: np.ndarray, title: str = \"\"):\n",
    "    \"\"\"Plot histogram of objects per image with maxDets thresholds.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    \n",
    "    # Histogram\n",
    "    ax.hist(objects_per_image, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    \n",
    "    # Add vertical lines for common maxDets values\n",
    "    maxDets_values = [10, 100, 200, 300]\n",
    "    colors = ['red', 'orange', 'green', 'purple']\n",
    "    \n",
    "    for md, color in zip(maxDets_values, colors):\n",
    "        ax.axvline(x=md, color=color, linestyle='--', linewidth=2, \n",
    "                   label=f'maxDets={md}')\n",
    "    \n",
    "    # Mark percentiles\n",
    "    p95 = np.percentile(objects_per_image, 95)\n",
    "    p99 = np.percentile(objects_per_image, 99)\n",
    "    ax.axvline(x=p95, color='cyan', linestyle=':', linewidth=2, label=f'P95={p95:.0f}')\n",
    "    ax.axvline(x=p99, color='magenta', linestyle=':', linewidth=2, label=f'P99={p99:.0f}')\n",
    "    \n",
    "    ax.set_xlabel('Liczba obiekt√≥w na obraz')\n",
    "    ax.set_ylabel('Liczba obraz√≥w')\n",
    "    ax.set_title(f'{title}\\nRozk≈Çad liczby obiekt√≥w per obraz')\n",
    "    ax.legend(loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print recommendation\n",
    "    max_objects = objects_per_image.max()\n",
    "    print(f\"\\nüìå REKOMENDACJA dla maxDets:\")\n",
    "    print(f\"   Max obiekt√≥w na obraz: {max_objects}\")\n",
    "    print(f\"   95 percentyl: {p95:.0f}\")\n",
    "    print(f\"   99 percentyl: {p99:.0f}\")\n",
    "    \n",
    "    if max_objects > 100:\n",
    "        recommended = int(np.ceil(max_objects / 50) * 50)  # Round up to 50\n",
    "        print(f\"\\n   ‚ö†Ô∏è maxDets=100 mo≈ºe byƒá za ma≈Ço!\")\n",
    "        print(f\"   ‚úÖ Zalecane: maxDets >= {recommended}\")\n",
    "        print(f\"\\n   W pycocotools ustaw: params.maxDets = [1, 10, {recommended}]\")\n",
    "    else:\n",
    "        print(f\"\\n   ‚úÖ Domy≈õlne maxDets=100 powinno wystarczyƒá\")\n",
    "\n",
    "\n",
    "# Analiza dla ARCADE dataset\n",
    "# Zmie≈Ñ ≈õcie≈ºkƒô na sw√≥j dataset\n",
    "ARCADE_COCO_PATH = \"./arcade_coco_detection/test/_annotations.coco.json\"\n",
    "\n",
    "try:\n",
    "    stats, objects_per_image = analyze_objects_per_image(ARCADE_COCO_PATH)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"üìä STATYSTYKI ARCADE DATASET\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"   Liczba obraz√≥w:     {stats['num_images']}\")\n",
    "    print(f\"   ≈ÅƒÖczna liczba obj.: {stats['total_objects']}\")\n",
    "    print(f\"   Min obj/obraz:      {stats['min']}\")\n",
    "    print(f\"   Max obj/obraz:      {stats['max']}\")\n",
    "    print(f\"   ≈örednia:            {stats['mean']:.1f}\")\n",
    "    print(f\"   Mediana:            {stats['median']:.0f}\")\n",
    "    print(f\"   Odch. std:          {stats['std']:.1f}\")\n",
    "    print(f\"   90 percentyl:       {stats['p90']:.0f}\")\n",
    "    print(f\"   95 percentyl:       {stats['p95']:.0f}\")\n",
    "    print(f\"   99 percentyl:       {stats['p99']:.0f}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    plot_objects_distribution(objects_per_image, \"ARCADE Coronary Vessels\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Nie mo≈ºna za≈Çadowaƒá: {ARCADE_COCO_PATH}\")\n",
    "    print(f\"   B≈ÇƒÖd: {e}\")\n",
    "    print(f\"   Zmie≈Ñ ≈õcie≈ºkƒô ARCADE_COCO_PATH na w≈Ça≈õciwƒÖ lokalizacjƒô\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå PODSUMOWANIE maxDets dla ARCADE:\n",
      "--------------------------------------------------\n",
      "‚Ä¢ ARCADE ma ~25-26 klas naczy≈Ñ wie≈Ñcowych\n",
      "‚Ä¢ Typowo 10-40 instancji na obraz\n",
      "‚Ä¢ Domy≈õlne maxDets=100 zazwyczaj wystarcza\n",
      "‚Ä¢ Je≈õli masz >100 obiekt√≥w/obraz ‚Üí zwiƒôksz do 200-300\n",
      "--------------------------------------------------\n",
      "\n",
      "‚úÖ Uruchom poprzedniƒÖ kom√≥rkƒô, aby zobaczyƒá rozk≈Çad dla Twojego datasetu\n"
     ]
    }
   ],
   "source": [
    "# üîß JAK ZMIENIƒÜ maxDets W PYCOCOTOOLS PODCZAS EWALUACJI\n",
    "# \n",
    "# RF-DETR u≈ºywa pycocotools do ewaluacji. Aby zmieniƒá maxDets, masz kilka opcji:\n",
    "\n",
    "# OPCJA 1: Modyfikacja po stworzeniu COCOeval\n",
    "# ------------------------------------------\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "def evaluate_with_custom_maxdets(\n",
    "    gt_coco_path: str,\n",
    "    pred_json_path: str, \n",
    "    max_dets: list = [1, 10, 300],\n",
    "    iou_type: str = \"bbox\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Ewaluacja COCO z niestandardowym maxDets.\n",
    "    \n",
    "    Args:\n",
    "        gt_coco_path: ≈öcie≈ºka do ground truth COCO JSON\n",
    "        pred_json_path: ≈öcie≈ºka do predikcji w formacie COCO results\n",
    "        max_dets: Lista warto≈õci maxDets [small, medium, large]\n",
    "        iou_type: \"bbox\" lub \"segm\"\n",
    "    \n",
    "    Returns:\n",
    "        COCOeval object with results\n",
    "    \"\"\"\n",
    "    # Za≈Çaduj GT i predictions\n",
    "    coco_gt = COCO(gt_coco_path)\n",
    "    coco_dt = coco_gt.loadRes(pred_json_path)\n",
    "    \n",
    "    # Stw√≥rz ewaluator\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, iouType=iou_type)\n",
    "    \n",
    "    # üîë KLUCZOWE: Zmie≈Ñ maxDets PRZED evaluate()\n",
    "    coco_eval.params.maxDets = max_dets\n",
    "    \n",
    "    # Uruchom ewaluacjƒô\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "    \n",
    "    return coco_eval\n",
    "\n",
    "\n",
    "# OPCJA 2: Dla RF-DETR - modyfikacja w kodzie biblioteki\n",
    "# -------------------------------------------------------\n",
    "# W pliku rfdetr/datasets/coco_eval.py znajd≈∫ CocoEvaluator.__init__\n",
    "# i zmie≈Ñ:\n",
    "#   self.coco_eval[iou_type].params.maxDets = [1, 10, 300]  # zamiast [1, 10, 100]\n",
    "\n",
    "# OPCJA 3: Monkey-patching przed treningiem\n",
    "# ------------------------------------------\n",
    "def patch_coco_eval_maxdets(max_dets: list = [1, 10, 300]):\n",
    "    \"\"\"\n",
    "    Monkey-patch pycocotools aby u≈ºywaƒá niestandardowego maxDets.\n",
    "    \n",
    "    Wywo≈Çaj PRZED treningiem modelu.\n",
    "    \"\"\"\n",
    "    from pycocotools import cocoeval\n",
    "    \n",
    "    original_init = cocoeval.COCOeval.__init__\n",
    "    \n",
    "    def patched_init(self, cocoGt=None, cocoDt=None, iouType='segm'):\n",
    "        original_init(self, cocoGt, cocoDt, iouType)\n",
    "        self.params.maxDets = max_dets\n",
    "        print(f\"üîß COCOeval.maxDets ustawione na: {max_dets}\")\n",
    "    \n",
    "    cocoeval.COCOeval.__init__ = patched_init\n",
    "    print(f\"‚úÖ Patched COCOeval z maxDets={max_dets}\")\n",
    "\n",
    "# U≈ºycie:\n",
    "# patch_coco_eval_maxdets([1, 10, 300])\n",
    "# model.train(...)  # Teraz ewaluacja u≈ºyje maxDets=[1, 10, 300]\n",
    "\n",
    "\n",
    "print(\"üìå PODSUMOWANIE maxDets dla ARCADE:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"‚Ä¢ ARCADE ma ~25-26 klas naczy≈Ñ wie≈Ñcowych\")\n",
    "print(\"‚Ä¢ Typowo 10-40 instancji na obraz\")\n",
    "print(\"‚Ä¢ Domy≈õlne maxDets=100 zazwyczaj wystarcza\")\n",
    "print(\"‚Ä¢ Je≈õli masz >100 obiekt√≥w/obraz ‚Üí zwiƒôksz do 200-300\")\n",
    "print(\"-\" * 50)\n",
    "print(\"\\n‚úÖ Uruchom poprzedniƒÖ kom√≥rkƒô, aby zobaczyƒá rozk≈Çad dla Twojego datasetu\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
